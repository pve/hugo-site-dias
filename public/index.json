[{"content":"It all starts with bits: digital units of information. It is about how we store these bits, how we move these bits, and how we transform or process these bits.\nThe three major elements of digital infrastructures are:\nStorage Networks Processors In the cloud security world, it is customary to talk about data at rest, data in motion, and data in use. This introduces data as the key common element. But more on cloud security later.\nAll of these elements have a wide variety of different manifestations, with associated quality and performance attributes that matter enormously in the design and management of digital infrastructures.\nYou\u0026rsquo;ll also have to understand that these three concepts are very entangled. A storage system is connected to a network, but it also has network and processing elements inside. The same is true for the other two concepts. Once again, it is a way to draw maps of digital systems, and any map is inaccurate. As said elsewhere, this inaccuracy actually can make it a good map! It reduces complexity so that we can more easily think about it.\n","permalink":"/book/diginfra/elements-of-digitalinfrastructures/","title":"Elements of the Digital World"},{"content":"What do roads, airports, the internet, the electricity network, and a search engine have in common? They are all services that are independent of a specific user or usage. They are provisioned on a longer timescale than that of an individual usage. And they are typically not owned by their users, or at least, not directly.\nIn this book I am mostly concerned with digital infrastructures. Their services are digitally accessible. Of the above examples, the internet and the search engine are the best examples of that. Interestingly, the other services increasingly rely on digital infrastructures themselves, or even incorporate specific digital infrastructures. An example of that are the trading platforms that enable the planning of electricity supply and demand. In many countries, electricity is a market, not a monopoly, which requires coordination between the various players.\nYou\u0026rsquo;ll also see that I talk about digital infrastructures as a service, not a product. It is not sufficient to buy a road, for example. For it to be truly useful to its users, it needs maintenance, repairs, incident management, and more. Any abandoned road will soon deteriorate without maintenance, and become useless.\nDigital infrastructures are sometimes called platforms. In the 1990s Windows was an example of a platform. A few decades later the concept of platform engineering started to be used for services that helped developers develop and deliver code faster. x It will be hard to define it with scientific rigor, in the same way that it is hard to unambiguously define what, for example, a car is. There will always be edge cases.\nWhat sets digital infrastructures apart in my opinion is that, thanks to automation, their scale, geographical reach and deployment speed can be orders of magnitude bigger than that of other infrastructures. That has a lot in interesting effects, which we\u0026rsquo;ll explore in this book.\nIn summary, digital infrastructures are about services that organisations deliver to independent consumers, while at the same time these services are subject to automation. That gives a tremendous dynamic in that relationship, and leads to new value, new power, and new risk.\n","permalink":"/book/diginfra/essence-digital-infrastructures/","title":"The Essence of Digital Infrastructures"},{"content":"In 2005 I visited my father in France. It turned out to be convenient to pick me up from the Gerona airport in Spain. And as we had enough time, we had lunch in La Jonquera.\nAt the restaurant\u0026rsquo;s checkout I noticed a peculiar array of devices: there were four payment terminals.\nI\u0026rsquo;d never seen that. From what I know, a merchant works with a bank which handles all their payments. Apparentely not over here.\nA closer inspection revealed that each was associated with a different bank. So apparently, depending on the bank that a customer of the restaurant works with, the restaurant needed to have a payment terminal associated with that.\nTo me, this does not sound optimal for the merchant. It takes money and deskspace to accomodate all those different payment networks. And if you have only one terminal many customers can\u0026rsquo;t pay by card. That won\u0026rsquo;t make them happy, and the last thing you want as a merchant is to have unhappy customers.\nAn online payment system, in this case with cards and terminals, is an infrastructure. and arguably a digital infrastructure. It serves many users: customers, merchants, and in this case the bank. One infrastructure for each bank.\nIt sounds obvious that there is a benefit in having a digital infrastructure like this that serves most banks, instead of just one. For example, you would not have to replicate installation and maintenance of those payment terminals. And indeed, in the current day and age, it seems to be the normal thing to do.\nBut apparently, it was hard to get to such a common system. So hard in fact, that it made sense for the banks and the merchants to waste all that money on replicating an infrastructure and foregoing all the benefits of a shared system.\nThere are a few hypotheses on why this is so.\nLet\u0026rsquo;s analyse this and see what causes this situation.\nFor one, services like these don\u0026rsquo;t appear out of thin air. They evolve. They don\u0026rsquo;t come into existence and remain stationary forever. There are actors and stakeholders that make them change.We\u0026rsquo;ll have to look at the actors involved and their objectives and possible actions if we want to understand how such systems look like at any given moment.\nThe main actors involved here are the bank(s), the merchants and the customers. When it comes to actual payments, these are the most important ones.\nThe banks probably expected to reduce costs and have more happy merchants and customers. And having this benefit earlier rather than later probably compensated for the higher cost of building out your own digital infrastructure for your merchants.\nWhile it would be cheaper overall to have a digital infrastructure that is shared across all banks, there seems to be a valid reason to not have that, at least not to begin with.\nMy hypotheses are: it takes time to agree on standards for interoperability and for actually setting up the service that is shared across banks. Not the least of your problems is who is going to own that infrastructure. It is likely to be a new legal entity. And that introduces a new stakeholder.\nNow back to the power dynamics that created the situation in the picture. Who benefits, who does not?\nThe banks are faced with a choice between the short term gain of realizing a payment infrastructure, and the long term gain of reducing its cost. And as they are investing, they have some power.\nThe merchant can chose between having multiple terminals, or sticking to just one, which amounts to a choice between spending money (and desk space) versus making customers unhappy.\nFinally, the choice for the customer is to pay cash or by card. The customer (and the other stakeholders) probably think that cards are a safer, and possibly cheaper alternative. The customer might therefore be led to feel that they prefer a merchant who accepts cards.\nThe banks and the merchant apparently prefer short term revenu increase over long term cost savings.\nAnd as a result the power dynamics play out so that it is the customer who benefits, the banks that invest and reap more benefits, and the merchant has to pay for a lot of that. Maybe the merchant will see some cost reduction in their payments handling.\n","permalink":"/book/diginfra/lunch-spain/","title":"What a lunch in Spain taught me about digital infrastructures"},{"content":"Deployment is everything that happens between writing software and actually using that software by its intended users. And as we get more software and more users, deployment becomes more complex.\nWhy deployment diagrams? Deployment diagrams are a great technique for communicating about important decisions in deploying software. Decisions such as who is going to do what, how are things connected, and so on.\nThere are many ways to draw deployment diagrams and many standards to choose from. UML and Archimate are just a few of them. To me, there is no single right way to create deployment diagrams. In that sense, these diagrams are more like maps. And the usefulness of a map depends on the journey that you are going to make. A map for a mountain walk is pretty useless if you want to make a railroad journey and vice versa.\nIn this section, I’m setting out to define the principles that these maps should adhere to.\nIt turns out that the actual symbols to use are not so relevant. It is more important to understand what we mean by those symbols, and how these diagrams are useful.\nWhat do we want to model? Modern IT systems are made up of many components, but all of them handle data in one way or another. Some of the important principles are that we want to understand where data is stored (at rest), where it is being processed, and how it moves from one place to another.\nWhat you want to explore is how data is connected because that tells us something about who can access which data. The converse of that is that we can also then see how things are isolated from one another. And this is a really important thing for security. If we know who is responsible for which component we can also understand who is responsible for capacity, for security, and for making it into production bringing it into production.\nWe used to think that we are connecting computers. But these days we are really connecting execution environments. An execution environment can be a computer, but it could also be a virtual machine or a container. Or it could be a Lambda function. The essence of an execution environment is that it runs code and that it is somewhat isolated from its environment. It also implies that we can then look at how these execution environments are connected.\nTake the following simple example. On the left we have a user working on the web browser, on the right-hand side, we see the web server. They are connected through the Internet. The web server is an execution environment that serves up webpages. The user’s browser is an execution environment that renders these webpages to the user. And we annotate the connection link with the protocol that is being used.\nSo we see two important concepts illustrated in this diagram: execution environments and connectivity.\nNow in a more formal notation such as UML or Archimate, this would look a bit different. Many would be able to argue about the specific symbols to use, but when I am pragmatic I am only concerned with how well the diagram communicates.\nHow detailed should your model be? A diagram is meant to communicate. It should communicate the most essential thing that you want to bring across to the people who are looking at your diagram. In this diagram, we are focusing on how the WebServer and the DNS service are probably different services. As a result, this diagram emphasizes the protocols that are being used, and the separate execution environments that are involved.\nAlternatively, we could introduce the Internet as an actor. In reality, by the way, the Internet is actually a set of actors, but we don’t show that here. What this diagram also shows, in comparison to the previous picture, is that there are multiple levels of detail on which you can draw these diagrams. Again, it all depends on what you want to illustrate.\nArchitectural decisions One of the purposes of deployment diagrams is to document and demonstrate architectural decisions. These decisions could, for example, be on the flow of data, the location of data, and where data is actually being processed. In the next example, we show how this works in a music distribution service, such as Spotify or Apple Music.\nIn your typical setup, there are three major components: the cloud-based music storage, the speaker or sound system to play the music on, and a mobile device to control it all from. This potentially gives us three connections. The first one is from the mobile device to the cloud-based music service to select the music that you want to play.\nBut how does the music get to the speaker?\nThere are two options. The first option is that the music goes from the cloud to the mobile, and then to the speaker. The other option is that the music goes straight from the cloud to the speaker. In fact, both options happen in reality. The first one describes a Bluetooth connection between the mobile and speaker, and the second one can be seen with Spotify connected speakers.\nIt is an interesting exercise to discuss the benefits and disadvantages of both options. You might also want to dive deeper into the actual protocols being used to make this work. Especially the protocols between the speakers and the rest are not so trivial.\nCloud-Native How is deployment different in a cloud-native age? To begin with, there is virtualization, there is containerization, we have functions as a service (such as AWS Lambda), we have DevOps, we have infrastructure as code, we have external services, APIs, and so on.\nAnd then we have also flexible infrastructure that can rapidly scale up and scaled-down. All of this brings complications to drawing deployment diagrams. Let’s see how we can accommodate each of these cloud-native concepts in our diagrams.\nVirtualization A virtual machine is an execution environment that lives within another execution environment, probably a physical host. Remember that an execution environment is isolated from other execution environments. A virtual machine is in complete control of the entire memory address space in the machine that it runs on. It only shares the hypervisor on that machine. That is quite powerful isolation. In fact, it also has network interfaces that are completely indistinguishable in capabilities from the network interfaces on a physical machine.\nSimilarly, a container (such as a Docker container) is an execution environment that lives within another environment. Each container is pretty isolated from the other containers that run on the same host, although not so isolated as a virtual machine is from another virtual machine. For example, containers share network port address space. This means that two containers on the same post cannot use the same IP address port number.\nIn the diagram below we have chosen not to show how the network is organized.\nNote that we have combined containers and VMs in one picture. This architecture actually happens in reality, it has its advantages. We’ve also chosen not to show any details of the underlying operating systems, hypervisors or Docker runtimes in this picture. Again, depending on what you want to communicate with the diagram, you might want to include those networking and other details.\nAutomated deployment Let’s move into slightly more advanced cases of deployment. It is pretty common these days to automate the deployment of software on any given machine. That has many advantages in terms of reproducibility, maintainability, and ease of upgrade. And that also comes with security benefits. However, it does require additional software and deployment components to get this going. That is because there are many more moving parts in this type of IT architecture.\nFor example, you need some kind of machine to deploy from, and a machine or environment to deploy to. You also need some technology to remotely deploy and configure the software. A popular tool for that these days is Ansible. Instead of typing commands on the command line, or clicking in a user interface, the configuration of the target machine is the result of a script that is executed. We say that this script is then ‘manifested’ into the target environment. As a side note, the word manifest originates from shipping, where it describes a document that lists the cargo items that are to be on the ship.\nIn automated deployment, it is very important to understand what type of infrastructure and software is created by whom or what. For that reason, we introduce a special type of relation, the manifestation relation. We signify this relation by a dashed line. Of course, there is still an underlying communication protocol that is necessary for this to work.\nIn the next diagram, the target machine is remotely configured from an Ansible playbook. On the target machine, we indicate the components and software that are to be installed there, and those should be described in the Ansible playbook. To make this a completely reproducible architecture, we also need to describe what the software components are that should be on the deployment machine to begin with. The old-fashioned flowchart symbol with the folded corner resembling a document is used to indicate where data is being stored. In this case, that is the Ansible playbook.\nCloud deployments In the previous diagram, we have simply installed the software onto an existing virtual machine. In a cloud world, however, we can go much beyond that. We can deploy what is called “infrastructure as code”, where complete architectures can be provisioned automatically. In the example below, we are using a deployment machine to run a Terraform script that automatically deploys 2 virtual machines and an internal network on the cloud provider. With the big oval, we indicate that there is some kind of ownership by the cloud provider of that part of the architecture, which also implies some isolation and control separation.\nThis diagram also shows how the Terraform script results in (is manifested to) actual deployment at the club provider. That is done through the cloud API.\nContinuous integration and delivery Next up in our journey towards ever deeper automation of software deployment, we have the continuous integration and delivery pipelines. As you might know, a continuous delivery pipeline takes software and other digital assets from a source code repository and builds it into something that can be deployed in an appropriate environment such as a cloud provider.\nThere are many examples of deployable artifacts. They could be AWS machine images, Docker images, or whatever you can think of. The artifacts are often stored in separate systems, across control boundaries. This is one of the reasons to put them outside of the pipeline.\nThe diagram shows the three major components, each of which could be under the control of a separate department or even organization. Note also how the diagram poses the question of how the build server (tagged ‘CI CD Pipeline’ in the picture) communicates with the software repository. In practice, there is some kind of trust relationship, where the build server is triggered whenever there is a commit to the repository.\nFrom the perspective of control, we see that this diagram identifies two pieces of data. On the right-hand side, we see the deployable artifact, and on the left-hand side, we see the source code repository. Different groups of people may have responsibility for these. We have taken the liberty to draw the deployable artifact as a kind of document, whereas the source code repository is depicted with the old-fashioned flowchart database symbol: the cylinder or disk. (As a completely irrelevant side note, this symbol might actually have been derived from magnetic drum memory, which ceased to be relevant in the 1960s).\nAt this point, we may feel the opportunity to combine these diagrams into one huge big diagram that allows us to see each and every detail of our full-blown architecture. It can be done of course. But often it is better to illustrate the architecture with a sequence of simple diagrams.\n","permalink":"/book/diginfra/deployment-diagrams/","title":"Deployment Diagrams"},{"content":"Most digital infrastructures are meant to communicate or coordinate, or are in support of other digital infrastructures that communicate or coordinate.\nThe internet is a prime example. It is designed to enable computers to communicate by moving data packets between them.\nSocial media is another example, used by people to communicate with each other. Its success is a testament to the fact that communication is a fundamental human need.\nFor an example of a supporting digital infrastructure you can look at an IaaS provider, which enables data processing and storage for other applications and infrastructures.\nFrom a business perspective this communication and coordination can lead to value in the form of, amongst others:\nbetter resource utilization and less waste, better process efficiency, better quality of services and products, reduction of risks. These are just the major, broad categories of delivering value, and there is considerable overlap between them. For example an organization\u0026rsquo;s website helps communicate to its customers. But you can also look at it as a more efficient way to organise the coordination on the delivery of products and services to the customer.\nAs you dive deeper into business processes, you will find that there is a wide range of processes and ways that lead to business value, each of which has its own peculiar ways of communicating and coordinating. In the early days of networking, I was sometimes asked what the value of a network is. Equally unanswerable is the question what the value of a message is. It just depends on the role that such a message plays in a process. For example, if your house is on fire, you only need a small message to reach the fire department, but if it does not get there, you\u0026rsquo;ll lose a house.\nStrategy consulting firms, like the one I worked at, do a lot of work in process reengineering to create value for their clients. The larger firms have serious collections of proces improvements. Over the past decades, significant reengineering of supply chains has taken place. The core idea of that is typically that better coordination reduces risk and inefficiencies.\n","permalink":"/book/value/value-in-process-improvement/","title":"Value in Process Improvement"},{"content":"Five elements of cloud security Historically, IT security started with infrastructure security. Just protecting the data center was good enough. But that was before we had data communications.\nWhen data started to escape the confines of the data center we needed to protect it. Typically through encryption. Hence we need data security.\nAs the world wide web developed, we saw applications being exposed to it, and frankly, be vulnerable. So that is when application security started to become more important.\nWith the proliferation of applications, both within the enterprises and outside of it, keeping track of user access became more important too. That is when we start talking more about user security (or identity management).\nFinally, as we are outsourcing to more and more individual software and cloud companies, we need to structure our oversight of those service relationships. That requires governance over contracts, SLAs, and more.\nCloud computing is having an impact on all of these five elements. Some are made easier, some increase in significance, some will change shape. Watch the video for more details.\nReview this video to learn more on these 5 elements of cloud security. Cloud security includes IT security as we know it, but will evolve to be more mature than that. In the video I elaborate on those 5 as they have evolved. Then I talk about how cloud changes our approach to those five elements.\nMore in other units.\n","permalink":"/posts/5-elements-of-cloud-security/","title":"5 Elements of Cloud Security"},{"content":"How cool would it be to let an AI do some of the grunt work in analysing the risk of applications and services. This has the potential to speed up the work of risk assessors.\nBut, does it work in practice? Well, here is an example of AI-assisted risk classification. I downloaded some of the entries in the Dutch algorithm register, which is a public register of systems that use algorithms. For each entry about 30 fields are available, including name, classification, owner, et cetera. Some of the systems in the registry are AI-based. Indeed, we have AI to help check on AI\u0026hellip;\nMy approach was to do some dialogue with Claude 3.7 Sonnet on a few cases. You can get the full description of each case from the algorithm register.\nMe: Here are three examples. Please return the name of the system, and your assessment if it uses an LLM or not.\nClaude:\nI\u0026rsquo;ll analyze each of these systems and classify them as either LLM-based or fixed algorithm-based.\nVirtual municipality assistant Gem\nClassification: Uses Machine Learning, but not an LLM Reasoning: The description states it uses \u0026ldquo;supervised machine learning\u0026rdquo; to match questions with answers from a predefined library. It calculates which existing answer best matches the question rather than generating new responses. This is more characteristic of a simpler ML classification system than an LLM. AI WBSO pre-advice\nClassification: Machine Learning, but not an LLM Reasoning: The system is described as a \u0026ldquo;self-learning Machine Learning model\u0026rdquo; that makes predictions based on specific variables to estimate risk. It appears to be a predictive model focused on risk assessment rather than language processing or generation, making it a non-LLM algorithm. Energy label generator (EP-Online)\nClassification: Fixed algorithm Reasoning: This system uses predefined formulas and calculations to generate energy labels based on specific inputs. The description mentions algorithms but refers to calculation procedures described in technical documentation rather than any learning models. This is clearly a deterministic system using fixed rules and formulas. None of these three systems appear to use Large Language Models. They each use different approaches: supervised machine learning for answer retrieval, predictive machine learning for risk assessment, and fixed algorithmic calculations for energy label generation.\nMe: Which of these processes personal data, just based on the description?\nClaude:\nBased on the descriptions provided, here\u0026rsquo;s my analysis of which systems process personal data:\nVirtual municipality assistant Gem\nProcesses personal data: Yes Reasoning: Although not explicitly stated, this system interacts directly with residents and businesses through chat functionality on municipal websites. It collects questions from users and monitors their feedback. The system likely processes some form of user identifiers to maintain conversation context and improve answer selection based on prior interactions. AI WBSO pre-advice\nProcesses personal data: No Reasoning: The description explicitly states \u0026ldquo;No personal data are used in the algorithm in any way.\u0026rdquo; It mentions using data like number of projects, hours, wages, etc., but specifically notes that only the case number is used as an identification label. A DPIA (Data Protection Impact Assessment) was carried out with the outcome that no personal data were processed. Energy label generator (EP-Online)\nProcesses personal data: Yes Reasoning: This system likely processes personal data as it generates energy labels for specific buildings owned by specific individuals. The fact that a DPIA was conducted (and is being updated) suggests personal data is involved. The system records data about buildings that can be linked to their owners, especially since the labels are required when selling or renting properties. Based solely on these descriptions, the Virtual municipality assistant Gem and the Energy label generator appear to process personal data, while the AI WBSO pre-advice system explicitly does not.\n(end of chat interaction)\nThis is a promising beginning. There are around 800 of these cases to review.\nInteresting followup questions either on the entries or on the approach could be:\nIs the description consistent with its classification? For example, there is a \u0026lsquo;risk\u0026rsquo; field, but does it match the rest of the description? Which entries seem to be missing crucial information? Which entries store specific types of data? Can we automate this a bit more? Stuffing the entries in a chatbot prompt does not scale very well. What is the accuracy, and will other models have similar accuracy? ","permalink":"/posts/ai-risk-classification/","title":"How AI Can Help IT Risk Classification"},{"content":"How to fix the WiFi? How to find a new phone for grandma?\nApplying technology follows certain rules. However, many people only have an intuitive understanding of these rules. As a result technology is not optimally applied. By understanding how applying technology really works, you can be more effective, more efficient, reduce waste, and overall do a better job. It takes just a few simple steps to improve any attempt at applying technology. We\u0026rsquo;ll focus on information technology here, though most principles have wider applicability.\nLet\u0026rsquo;s start by looking at a few examples that I have used these in teaching IT students. The first example is fixing the WiFi at home, and the second one is selecting a new mobile phone for grandma. Think for a moment how you would approach these two cases. For a more corporate example you can think of replacing a content management system, or introducing a new security capability.\nThe core idea of technology is that by doing an intervention in the real world, you achieve certain desirable outcomes. This is in contract to the core idea of science, which is that by doing observations on the real world, we can derive certain conclusions that allow us to predict what might happen under similar conditions in the future. Science seeks to understand and explain the underlying principles governing phenomena, while technology applies that understanding to create tools, systems, or processes that achieve specific goals.\nOf course, science and technology are intertwined: technology relies on scientific insights to be more effective, while science often needs technology in order to do experiments and observations.\nAs our ambition is to apply technology in a bit more systematic way, let us begin to ask ourselves what it is that we try to achieve.\nNote to self: technology literally means the science of technics. In english there is not much of a distinction between the techniques and technology.\nA technique aims to achieve a certain desirable outcome. Interestingly enough, the attention is often on the technique, and a little less on the outcome. I find that focussing on the outcome first is a productive and relevant thought exercise.\nLet\u0026rsquo;s review our examples. What does fixing the WiFi mean? What result does it achieve? This could be many things. Maybe we want a better internet experience in the back of our garden, or on the top floor? Maybe we want to have higher internet speeds inside the house. Maybe we want less packet loss. Maybe we want to play our games with less ping delay, so we can win more often. Maybe we want to allow our friends easier access, or more security.\nThe essence of the result is that some measurement of quality improves for some stakeholder.\nTurning our attention to grandma\u0026rsquo;s new phone. What do we want to be improved? Maybe her current phone is too hard to navigate because the buttons are too small, or it does not run an app that allows here to see the pictures of her grandchildren. Or maybe it breaks down all the time, and your tech savvy nephew has to come over to fix it, and he does not want to do that as often anymore.\nNow we can turn our attention to the intervention. In our WiFi case, are we buying more access points, and if so, where should they go, and how should they be connected? Maybe we want to change the frequency allocations, or allow use of the 5GHz frequency bands. With grandmas phone, do we install new apps, buy a different size phone, or what?\nHow does the intervention lead to the desired result? In other words: how does one lead to the other? Maybe you have a theory about that. Maybe you\u0026rsquo;ll just need to figure it out. In both cases you start with a hypothesis that explains how the intervention leads to a result.\nFor example: because WiFi signal strength (and therefore quality) decreases with distance between device and base station, we install more base stations in order to have better quality around the house. Another example would be: because there is limited frequency spectrum, we reconfigure our existing base stations to use different frequencies in order for them to interfere less, and have better network quality within their range.\nAs you can see from this example, your assumptions, your hypotheses, matter.\nWe can morph these statements in a variety of useful ways. This is what makes for professional application of technology. Here are some variations.\nBy theory Z, we do intervention X to achieve result Y.\nWe hypothesize that intervention X will lead to result Y, in which case we can start to form a theory.\nHow does intervention X lead to desirable outcomes?\nWe are looking to achieve result Y, and we have theory Z. What does that tell us about possible interventions?\nOnce we grasp this, we can move on to more complicated scenarios. We can flesh out what a better outcome is. We can use this to do design in more complicated situations such as with more stakeholders. We can also use this to do more systematic troubleshooting.\nMore about that later.\n","permalink":"/posts/technical-meta-model/","title":"Applying technology by following a system"},{"content":"The international arena has many actors that can influence digital infrastructures. But opinions differ on what the important ones are, or even what the relevant ones are. By the nature of scaling, very few digital infrastructures are influenced by national actors only. The international arena is complex, and there are even conflicting views on how it is made up. These conflicting views can best be seen as maps of the territory. And as I have mentioned earlier, a map is only relevant for a specific purpose.\nThe first category of actors is nations, obviously. The word nation is part of the word international. But we can see many other actors exerting influence, such as multinational corporations, NGOs (non-governmental organisations), treaty organisations, and so on.\nNations make laws, multinational corporations make investments and offer products, NGOs such as ISO make standards, treaty organisations such as the EU make more standards and regulations. And sometimes these are conflicting.\nNot everybody shares this multifaceted view of the international arena. Some hold that nation states are the only actors that have real power. After all, they control the territory on which people live, and have a monopoly on military force and a legal monopoly on violence.\nThrough law and agreements within and between nations, the other actors are enabled. Multinational corporations don\u0026rsquo;t really exist in this view. They are constructs enabled by agreements between nations, for example on recognizing property across borders. How is it possible that a national of one country can hold property in another country? This is not universally the case. Many countries do not allow this, at least not unlimited.\nA multinational corporation, even if it has public shareholders across the globe, has a primary residence somewhere. And the legal options and obligations trickle down from there.\nSimilarly, NGOs such as ISO (International Standards Organisations) and the International Criminal Court are also established somewhere, and recognized by other countries, or not, as the case may be.\nDigital infrastructures don\u0026rsquo;t necessarily map easily to a single organisation running them, or even governing them. The internet is a interesting and complicated example. On a day to day basis, internet service is largely provided by telecommunications companies owning the cables and radio base stations. Administering the coordination between these and other providers, for example on IP adresses, is done by a non-profit established in the US. But there are many more players in that space, often independent.\nIn contrast to the multi nation view of the world is the perspective that what really matters in the international arena are the multinational corporations. They have significant control over the international flow of goods, services and investments. Understand that many of these corporations have annual turnovers that are larger than the gross domestic product of 80% of the world\u0026rsquo;s countries.\nLarge businesses have the capability to drive government policies. Even US presidents see that. President Eisenhower warned about the military-industrial complex in 1961. President Biden extended that, and warned against the power of the tech industry and AI in particular in his last speech in 2024.\nThis is not hypothetical. There has been succesful lobbying by US companies against international regulation that is seen as limiting to them.\nMany multinational organisations are largely legitimate, but there are quite a few whose main objective is to launder money outside of the control of nations. Beyond that, when it comes to cybersecurity, internationally organised crime has business models that include ransomware extortion and bitcoin mining on hacked computers.\nThen there is the perspective that the only really important international actor is the US. With more than 700 military bases in more than 80 countries, this is the single largest international military presence in the history of mankind. Additionally, since the end of world war II, the US has alledgedly intervened in dozens of countries, through military or other means, three times more than Russia, the next contender.\nFinally, it is conceivable that there are people who think China has always been, and will be, the economic superpower of the world, with a brief 200 year interuption which is about to end.\n","permalink":"/posts/international-actors/","title":"International Actors"},{"content":"Who is really doing Zero Trust?\nWell, the US Air Force is. Here is my summary, with some comments, of their strategy document for the benefit of my Certificate of Competence in Zero Trust (CCZT) learners. In fact, this is an edited version of a conversation we had during one of our classes.\nYou can download the full strategy here, and the current roadmap here, all linked from this page.\nI think the fundamental first important point about the strategy document is that it exists at all. There is an actual organization of significant size that has a strategy and is implementing it. Many can learn from this.\nThe second major point is that its terminology and even its conceptual frame of reference is almost 100% CCZT compatible. Even better, it lists more specific initiatives than the Zero Trust courseware itself. So rather than talking about a strategy, it actually is a strategy with strategic goals and sub-goals that match.\nI am not repeating the strategic goals and subgoals here. They align with CCZT, and the strategy document is succinct enough.\nData is considered to be the strategic asset to protect. It is really focussed on protecting the data. Side note: for regular organizations, it is actually quite hard to have a top-down understanding of the value of data. Anyway, the focus on data represents a shift away from a network centric security strategy. In fact, the current strategy is defined as network focused, defense in depth, castle and moat.\nBenefits Already in the summary it mentions three important benefits of the strategy, instead of just saying \u0026ldquo;We need to be more secure\u0026rdquo;.\nReduction of architectures Unlock access Impose higher costs on adversaries The strategy summary states that it reduces the number of architectures. Later in the document, it\u0026rsquo;s a bit more specific about that, saying there are too many point solutions and stop-gap measures across that entire network. This is what happens - you start with a low-security network and then as you mature your requirements, if you don\u0026rsquo;t have an overall strategy, you get all these point solutions and they are expensive simply because you have so many different ones. So the zero trust strategy has, as one benefit, that you will have less of these.\nIn IT, migrating from a functioning system to a new one rarely has a strong business case. As a matter of fact, that\u0026rsquo;s true for a lot of technology. If it ain\u0026rsquo;t broke, don\u0026rsquo;t fix it. That\u0026rsquo;s for a number of reasons. One of them is the business case: it is too costly. But if you\u0026rsquo;re not happy with the current situation, you have a change imperative and the business case changes because your migration cost is a sunk cost and no longer something that needs to be recouped.\nThe most important functional benefit mentioned is that it unlocks access. And a little later it explains: \u0026ldquo;to shared and trusted information.\u0026rdquo; This is enabling warfighters. Well, also a lot of other roles. But the mission of the air force is to fight, or at least demonstrate the capability to fight, which is probably even more important than the actual fighting. It\u0026rsquo;s fairly obvious that you want to do this in quite adverse network circumstances. They even have an abbreviation for that - DDIL (denied, degraded, intermittent, and limited) - for situations where you may or may not have a connection to your drone, for example.\nThe third one that I liked from a security mindset perspective is that it imposes higher costs on adversaries. They don\u0026rsquo;t say we need to protect ourselves from adversaries, because that is not a goal that has a measurable KPI. It\u0026rsquo;s hard to say \u0026ldquo;okay, we\u0026rsquo;ve reduced our exposure to adversaries.\u0026rdquo; The adversaries are all over the place. In fact, without adversaries, you wouldn\u0026rsquo;t need to have an air force.\nSo what they\u0026rsquo;re saying is, okay, our defensive strategy is based on increasing the cost of the attacker to attack us. This is military doctrine - you end a war when the enemy no longer wants to fight you. That\u0026rsquo;s the military definition of ending a war, dating back to Sun Tzu. One way of doing that is raising the cost for them to continue attacking you.\nSo I liked these benefits very much. I think they\u0026rsquo;re understandable on a strategic level.\nScope In terms of coverage, the strategy sometimes looks like buzzword bingo. First of all, it mentions all new developments like AI and IoT. It includes top secret data and operational technology. Cloud and mobile is included, even non-IP devices. Microservice architectures.\nIt acknowledges that more endpoints are moving to the cloud.\nOld is new again? The strategy contains a lot of elements that you already see in mobile security infrastructures that are being build these days. Mobile, cloud, \u0026rsquo;new way of working'.\nIt is a theme that we\u0026rsquo;ve seen before - zero trust isn\u0026rsquo;t about reinventing the wheel. It is not about reinventing all great tactical security stuff we already know. Most of that is still useful.\nZero trust is a more comprehensive and consistent approach to stuff that we already know.\nPotentially a new technology that is explicitly mentioned is SDP (software defined perimeter). But even that has its roots in earlier technologies such as the knock knock protocol. SDP is a more structured variant of that.\nSDP is fairly directly stated as a tool to eliminate VPNs. This is a substantial driver of the benefits mentioned.\nMaturity and automation Section three includes a small maturity model, in which an important dimension is \u0026lsquo;automation\u0026rsquo;. In the document, the mantra is repeated regularly: we do this initially with manual efforts but we must evolve to a real-time and automated approach. I think the nice thing here is that it acknowledges that you need to get started on it and don\u0026rsquo;t overautomate in the beginning, but that manual processes are not sustainable in the long run.\nYou don\u0026rsquo;t want to automate what you don\u0026rsquo;t understand. So there\u0026rsquo;s a fine line there. As a matter of fact, I\u0026rsquo;ve written earlier about this in the context of automating system management and network management. And one of my former colleagues had a PhD thesis of which that was the most important result.\nThe big elephant Another interesting point, the ramifications of which are not immediately clear, is that without mentioning Microsoft, the document states: \u0026lsquo;We have Active Directory Domain Controllers all over the bleeping place.\nThis is the Microsoft Active Directory architecture where every location has its own domain controllers - and this is archaic because this is from the period that you had flaky connections to offices. In the old time at banks where I worked, every office had an active directory domain controller. In fact, two because of high availability requirements. But it\u0026rsquo;s an attack surface. The Air Force says we have over 180 sites, so we have more than 360 attack surfaces.\nOne of the stated objectives of the strategy is to drop the dependence on these Domain Controllers. Now, how they are actually going to do that, it doesn\u0026rsquo;t say, but I thought it was a really interesting point to make.\nLogging and shared responsibility What I liked also is the emphasis on comprehensive security event monitoring and logging, including application security events.\nThey also highlight the need for a \u0026lsquo;shared responsibility model\u0026rsquo; for handling those events, involving both external security service providers and internal security operations centers. I think that\u0026rsquo;s a nice frame of mind or a nice thinking model on how to roll that into your architecture.\nGovernance and risk Strategy can be a theoretical exercise. How do you pull this off in reality? The final part of the paper prescribes how the responsibilities for the implementation are delegated to various offices and units within the Air Force organization. A dozen of them are mentioned, and it even spells out where some of the deconflicting of priorities is going to happen. Deconflicting is a core element of governance - that you find out: where are the conflicts and how do we actually handle them?\nInstitutional resistance to change is identified as the greatest risk, as it requires \u0026lsquo;all communities to adapt in uncomfortable ways.\u0026rsquo;\u0026quot;\nAmen.\nOpen ends In a couple of places, it says we need to do discovery of everything on the AFIN - the Air Force Information Network. And then somewhere else, it says we have a cloud-first priority. My question: wait a minute, are those cloud services part of the Air Force Information Network or not? It depends a little bit on your definition of what a network is. This is, by the way, not new for this document. I\u0026rsquo;ve seen that ambiguity over the past 30 years in a couple of other ways and places. Is the network the stuff that routes packets? Is it everything that you can reach from a certain device? Is it all the servers? Does it include all the clients?\nIt\u0026rsquo;s a little less clear how that works. So there are some loose ends on the edge of the strategy.\nWhat I also did not see is how progress is going to be measured, and how you would audit or assess that.\nIn conclusion: read this if you are interested in enhancing your IT-based capabilities while reducing your security costs.\n","permalink":"/posts/airforce-zero-trust-strategy/","title":"The US Air Force Zero Trust Strategy"},{"content":"Risk: LMS 3-2\nCERN data center\nverhaal over corporate en MKB\nHow technical debt works: negative investment externalities.\nBut of course you have this discussion on shared services. As a matter of fact, I keep talking about the book that I\u0026rsquo;m probably not going to finish, but it\u0026rsquo;s still a nice frame of reference. Now digital infrastructures at scale - there are a few very essential elements in there. The most important elements are: one, that you have a separation between a provider and multiple consumers; and two, that that service is digital, which means that it\u0026rsquo;s amenable to automation. These two together give a really interesting socio-political economic dynamic. And that\u0026rsquo;s what you need to understand if you want your shared service to keep operating - or if you don\u0026rsquo;t want it to operate, but that\u0026rsquo;s a different story.\nIkigai\nOnderwerp: power, vendor lock-in\nDe shared services business case. Voorbeeld Haven ook (journal feb 2025). Lees ook input van Robert (?)\n","permalink":"/snippets/","title":"Snippets"},{"content":"Agile development is all the fashion nowadays. Why is that and what kind of digital infrastructures does that require? Back in the old days, business software was primarily written to automate existing business processes. Those processes might change somewhat as a result, but in the core processes were no different. Think accounting systems, scheduling, \u0026ldquo;customer relationship management\u0026rdquo; and so on.\nToday we see that software not only automates these business processes, but becomes part of the product, or even becomes the product itself. And on top of that, this software is often an on-line service. Think of the smart room thermostat. Or financial services, where software increasingly is the main part of the product: think online banking. And in social media from Facebook to Tindr, software really is the product.\nThe dance Every product changes the world that uses it. Think how cars have changed the way people commute, or even choose where they live. Software is no different. But a changing world also changes the products we can use or want to use. There is a kind of dance between supply and demand. Do we have breakfast out of our house more often because there are more opportunities for this or does the supply of breakfast places increase as a result of us going out more? Just as in a dance, it is not always easy to tell who is leading who.\nBecause software now has become the product it will also participate in the dance, and then it becomes more important to quickly adapt to the dance partner. As a developer, you change the world that uses your software in ways you cannot always predict, so in reaction you have to adapt to that world.\nThe faster the better.\nThis explains the need for agile development. Between idea and realization (time to market) there should not be two years, but only two weeks, and preferably less.\nWhat kind of digital infrastructures does that require? The prime objective of digital infrastructures is to enable the deployment of application functionality. The quality of digital infrastructures used to be measured in the number of users it could support well. I.e. we used to talk about a system supporting 10.000 concurrent users with less than 4 seconds response time.\nBut agile development comes with a new goal: \u0026lsquo;feature velocity’. That is the speed with which new features can be deployed. The time between inception of a feature and its deployment to a large user base has to be shorter than the time it takes for the environment to change. In a dance you want to anticipate your partner, not stand on her toes.\nYour digital infrastructure should not be a bottleneck. This requires features such as automated testing, quick scaling up and down of resources, and as little manual intervention as possible. This is the only way to shorten the lead time for a change.\nCloud computing In summary: agile development requires cloud computing. Remember: the essential characteristics of cloud computing include fast, elastic and self-service provisioning of resources. That is what is required for agile development.\nAnd then the dance continues. Because if we can do that, we can do other new things. Like better security. If you can respond quicker to new functional requirements, you can also respond quicker to security issues.\n","permalink":"/book/diginfra/agile-requires-cloud/","title":"Agile Requires Cloud"},{"content":"In a digital world, it is not always easy to think through what risk is. While we can borrow a lot from the real world, certain important differences exist.\nAt the core of every risk assessment there is the thing we worry about the most: the \u0026lsquo;asset\u0026rsquo;.\nIn a digital world, this is often the data. Think business critical data, like our database of customers. Think data that we have a compliance obligation on, such as personal data.\nIn information security, like the name implies, we mainly worry about the security of that data.\nIt is common to distinguish between availability, confidentiality, and integrity risks. All of these can be a risk to the business.\nBusiness processes run on data, and if they are not, they can typically improved by using more data. We cover that in more detail in another place.\nIf the data is not available, the business process can suffer, and its value reduced.\nIf the data leaks out, the business process can suffer, and its value reduced.\nIf the data does not reflect the reality well enough, the business process can suffer, and its value reduced.\n(examples of all)\nTo be continued.\nwhat other assets? who suffers? externalities.\n","permalink":"/book/risk/intro-to-risk/","title":"Intro to Risk"},{"content":"The sheer separation of the providers from the consumers of these infrastructures drives a lot of the value.\nImagine stringing a wire to connect your PC to your neighbour\u0026rsquo;s PC in order to play a game together. People have been doing these things. How long does that take? What is the cost? Where do you buy the cable? How are you going to run that cable? It is a lot faster to just use the internet for that.\nThe introduction of a separate provider changes the provisioning process. For one thing, the provisioning can go much faster. The provider has capacity (stock so to say) that can just be used. Related to that is that the service is already defined by the provider. Instead of designing it from scratch, you just pick one of the options offered. Presumably, that option has had a lot of design and thinking behind it already. It is likely to be a better option than anything that you can think of yourself.\nSo, that brings value: it can be faster and better than your homegrown alternatives.\nBecause the provider has multiple consumers, there are economies of scale that play out. Whole books have been written on this, and we will be looking into that in more detail. Think of sharing of unused capacity. Not everybody is using their internet link to the max all the time.\nBut \u0026lsquo;digital\u0026rsquo; brings more specific benefits. It is the opportunity to automate service delivery that has immense effects on economies of scale. Think about it: if a piece of software does its work, replicating that piece of software across a lot of servers is a neglible amount of effort in comparison to creating that software, typically. In contrast, baking 1000 pizzas is a lot more work than just baking one or two.\nFinally, consumers can find value in being connected with other consumers. The value of a phone or social network to the individual users, not just the network owner, increases with the number of other users connected to it. As a result the value of the network increases quadratically with the number of users. This has implications for the power balance, as we will discuss later.\nIn a later unit we\u0026rsquo;ll discuss some of the words that economists use for these phenomena, such as investment externalities and network externalities.\n","permalink":"/book/value/intro-to-value/","title":"Provider Value"},{"content":"This book is in progress! Here are the chapters that have had blog posts promoted to them.\nChapters:\nPreface Introduction Essentials Value Power Risk Cases Wrapping up Not all chapters are finalized. Blog articles might be included later!\n","permalink":"/book/landing/","title":"Book contents"},{"content":"Here is the draft design of the graphic novel version of this book, which may or may not happen..\n","permalink":"/posts/graphic-novel-page1/","title":"Digital Infrastructures - the graphic novel"},{"content":"One of the key aspirations of Digital Infrastructures at Scale is to equip you with the tools to shape and drive change in your professional environment—especially when your goal is to lead a transformation.\nTwo important positions of influence in IT are architects and auditors/assessors.\nIT architects are builders. They design new applications, platforms, and infrastructures that enable businesses to operate more effectively. A CRM system, for example, is not just a technical solution—it transforms workflows, communication, and decision-making. Similarly, infrastructure architects create digital foundations that accelerate the deployment of such applications.\nAuditors and risk assessors, on the other hand, focus on preventing failure. Every technology introduces risks, and without proper attention, these risks can derail progress—whether through security breaches, compliance fines, or operational breakdowns.\nBoth roles involve power. Without power, things will not move in the way you want, and every initiative encounters resistance. Understanding the sources of power—and the forces that push back—is essential for you to achieve meaningful outcomes.\nMoney is a form of power, but it is often a blunt instrument. In organizations, other forces—such as survival and strategic priorities—can be even more influential. The key to driving transformation is linking these forces together with precision.\nThis book provides you with the frameworks, insights, and real-world examples to navigate these challenges and drive successful change.\n","permalink":"/book/power/positions-of-power-it/","title":"Positions of Power in IT"},{"content":"The AI landscape has many digital infrastructures.\nLet\u0026rsquo;s explain this step by step and focus on which data is stored where, and how it is processed.\nA core element of AI systems is a trained model. At least that is true for the dominant AI form these days: deep learning neural networks.\nA trained model is the result of processing a lot of training data by a specific neural network. These models are fixed in size, but typically very big. The smallest useful models are close to a Gigabyte, while recent public chat models run into multiple Terabytes.\nA trained language model is effectively a piece of software. You feed it text, and text comes out. This is called inferencing, and it requires that the entire model fit into RAM memory. Only for the most minimal cases is this possible on a high end laptop. For more interesting use cases you\u0026rsquo;ll need a lot more hardware. Therefore there is a good business case for sharing this across many users.\nblock-beta space in space llm:2 space out space in[\u0026#34;Prompt\u0026#34;] --\u0026gt; llm[\u0026#34;Large Language Model\u0026#34;] llm--\u0026gt; out[\u0026#34;Completion\u0026#34;] style in fill:#fff,color:#000,line:#000;stroke-width:0px,color:#000,stroke-dasharray: 5 5 style out fill:#fff,color:#000,line:#000;stroke-width:0px,color:#000,stroke-dasharray: 5 5 In cloud terms, we would call this Inferencing as a Service. In the NIST model, we\u0026rsquo;d call this SaaS or PaaS, depending on whether is it consumed directly or as part of a larger application.\nThis gives us the option to look for this on the market, or create a shared service inside an organisation or group of organisations (e.g. private / community cloud). It also implies that we need to have a clear definition of the service and related service level objectives. This includes any allocation of security responsibilies.\n","permalink":"/book/diginfra/digital-inf-ai/","title":"What are AI digital infrastructures?"},{"content":"Building Our Own Cloud Kootwijk: Rethinking Digital Sovereignty In the Netherlands, we are currently engaged in a heated debate about the undesirable dominance of big tech, particularly over a significant portion of the digital infrastructure of the Dutch government. This includes email, file storage, and many other forms of digital storage and processing—most of which are handled by American big tech companies.\nI am sure a similar debate is going on in many other countries.\nGiven this reality, what can we do about it? How can we build our own \u0026ldquo;Cloud Kootwijk\u0026rdquo;?\nExploring Alternatives Setting up a comprehensive, government-funded solution does not seem like a viable approach. History has shown many failures in such attempts. Moreover, our current procurement processes do not lend themselves to agility. Due to the scale involved, any such initiative would likely end up with a large, well-established company.\nThis brings us to another challenge: every big tech company is, or ultimately becomes, an American company. The two largest software markets in the world are the U.S. and Germany. Even the largest German IT firms cannot afford to ignore the American market and are often listed on U.S. stock exchanges.\nAs a result, every big tech company—directly or indirectly—falls under the control of the U.S. government, regardless of its political orientation at any given time.\nRethinking the Approach With this in mind, we need to rethink our strategy. If big tech is not the answer, how can we organize \u0026ldquo;small tech\u0026rdquo; in a way that makes it a viable alternative for a large-scale buyer?\nWe are talking about digital infrastructures that are essential to the functioning of our society. These infrastructures have a certain \u0026ldquo;commodity\u0026rdquo; nature and do not primarily revolve around innovative applications. In principle, they should be deliverable by multiple companies.\nThere is no fundamental barrier to IT services being provided by a consortium of service providers. In fact, this is how the Internet has operated for decades. Its core consists of open standards that facilitate a market of service providers with sustainable business models. To prevent monopolization, these markets need some level of regulation, including enforcing open standards for interoperability and portability. In the case of strategic autonomy, additional rules regarding ownership of these companies may also be necessary.\nWhile having much of the underlying software available as open source would help, it is not essential. Open source is neither a sufficient nor a necessary condition for interoperability and portability.\nOpen standards (ideally with open-source reference implementations) are more important. They also contribute to another critical element of \u0026ldquo;small tech\u0026rdquo;: building sufficient expertise.\nKnowledge as the Foundation of Autonomy In my view, IT services consist of hardware, software, and systems management . Management can be broken down into processes and skilled personnel. Most of these elements are either commodities or readily available.\nFinding enough personnel competent in the relevant technologies is more challenging, but not impossible.\nTo achieve a sufficient level of autonomy in digital infrastructures, we need to develop and maintain this expertise. The best way I know to do this is to start small and scale up. Expanding by a factor of ten at each step is an ambitious yet achievable goal. This points towards managing our digital infrastructure through a consortium of smaller companies. The VNG’s Common Ground and Haven initiatives offer promising examples in this direction.\nA Call to Action I challenge both policymakers and society to launch a production pilot using a fully open-source office automation environment. I believe we could select the necessary open-source components for such a pilot in an afternoon. Managing this at a scale of 1,000 active users would be an excellent starting ambition. This will take months—perhaps longer—but if successful, it will provide a blueprint for a \u0026ldquo;Cloud Kootwijk\u0026rdquo; should the need become urgent.\nWill this be painful? Yes. But not as painful as realizing too late that we have lost our democracy and prosperity to a foreign power—or worse, to a group of multinational corporations that are not even under the control of any state.\n","permalink":"/posts/how-can-we-achieve-autonomy/","title":"How Can We Achieve Autonomy Over Our Digital Infrastructures?"},{"content":"This is hardly a book on digital technology, but more on the organisational and societal impact of managing this at scale.\nFor a better understanding of digital technology I highly recommend Brian Kernighan’s “Understanding the Digital World”. () And if you want to have fun while learning how to understand software, I’d highly recommend “The Nature of Code” by Daniel Schiffman ().\nAs you will find out in this book, I am putting on some very specific lenses to look at the world. You may find them restrictive; you may find them arbitrary(?), you may find them opiniated. Yes, this specific way of looking at the effects of digital technology has its limits. However, the essence of understanding is to be able to extract the core elements out of something. Paradoxically, only by restricting the way we look at things can we see more clearly. Cut through the noise. Like Goethe said: “In der Beschränkung zeigt sich erst der Meister”, though he applied that to a different subject.\nI have a very limited perspective of the world. There are so many things and concepts that I just fail to remember or understand. And yet, that limitation has served me well. It has allowed me to focus on core principles that you may not see if you take in all the richness and idiosyncrasies. My limited perspective can serve you as a map. And you know that every map is not equal to the terrain it covers. In fact, the purpose of the map is to extract only the most relevant features of the terrain. How good a map is, judgement on the quality of the map is not objective. It is dependent on what you use the map for. A railroad map is close to useless if you navigate by car or boat. That is why you should consider the models and the perspectives that I give you as maps to guide your journey. If they bring you where you want to be, great! If not, get a different map.\nAs you will see, the core elements of the models I find useful revolve around interactions between autonomous agents that process information. These agents typically have limited (bounded) capabilities. (link to maps as means to reduce attention requirements). These agents can be humans, software or AI agents.\nAs I believe that you are an autonomous agent as well, it is up to you to decide how useful these models and stories are for your practice. You will not find answers to every question that you have around digital infrastructures. Where possible and relevant I will point you to other views, stories and explanations. At the same time, you may find my maps to be also useful for domains that are outside the digital.\n","permalink":"/book/preface/preface/","title":"Preface"},{"content":"Zero Trust Architecture is an approach to better cybersecurity. To many, it seems daunting to implement. But it does not have to be hard to start.\nConsider this hypothetical situation.\nYou have an application with hundreds of thousands of sensitive records, let’s say client records. We assume that in this example it seems hard to implement MFA (Multi Factor Authentication) on it. What other controls can you implement to reduce the assumed trust? We can use the Kipling method, which is at the core of Zero Trust architectures, to engineer better controls. In short, the Kipling method is about the \u0026lsquo;who\u0026rsquo;, \u0026lsquo;what\u0026rsquo;, \u0026lsquo;when\u0026rsquo;, etcetera of allowed communication.\narchitecture-beta group exec(server)[Execution environment] service users(cloud)[Cloud Users] service fw[Firewall] service app(database)[App] in exec users:R --\u0026gt; L:fw fw:R --\u0026gt; L:app We want to allow specific access for specific use cases and be explicit about it. However, to focus our efforts, it makes sense to also identify that our biggest risk is the exfiltration of a lot of those sensitive records: a data breach.\nThe allow rules Let’s begin with the ‘who.’ Who is accessing the information? The application in our example probably does authentication and authorization of users. Where do these come from, and how accurate is that information? Ideally, this comes from an up-to-date corporate directory, but even if it’s not, you can, for example, ask how quickly a departing user has their access revoked.\nThen the ‘what’. Which application and resources are we accessing? What are specific users allowed to do? As part of operational risk management, we have probably already identified who, based on their role, can modify records or do other critical actions. But think one step further. As our main fear is exfiltration of large amounts of data, we can look at where we can control that. Maybe we can disallow large downloads, for example, except when a timely ‘four eyes’ approval is in place.\n(By the way, I think it makes sense to look at large downloads as a risk separate from small data leaks. After all, many employees have access to small amounts of information, and that risk is typically already accepted).\nMoving on to the ‘when’. It can reduce risk to limit access to specific times of the day for most users. There is not necessarily a reason to allow more.\nNext is the ‘where’. Where are the users and the application located? Our case description does not give us a lot of information here. The application could be a server, which then has an IP address. And through a firewall combined with a geographical information feed, we may be able to restrict access to users based on their location.\nThe ‘why’ stands for the business reason. In this case, our data is sensitive, and we don’t want it to be exfiltrated. And in our analysis, we have identified several possible controls.\nHow: One layer of enforcement of this would be in the application itself. In the Zero Trust jargon, we are putting a Policy Enforcement Point in the application logic.\nThe mirror allow rule However, there is also another set of ‘allow rules’ that is often overlooked. I am tempted to call these the ‘mirror’ rules, but it is not a standard Zero Trust term.\nThe data and the application reside in a compute environment, for example a server. There may be malware running in that server. After all, one of the tenets of Zero Trust is ‘assume breach’. If the server has permission to access anything on the internet, that malware might easily exfiltrate large amounts of data.\nIn addition to looking at the user accessing the data, we are looking at the place where the data resides and see if its compute environment can access the world, and thus exfiltrate data. This is the mirror image of the first rule: you could say that subject and target are switched around.\nWe can apply the same Kipling method from the perspective of that server (or any compute environment that has the data in it). What is it allowed to do?\nThe ‘who’ then is the server, and there are various ways to identify it, depending on how it is set up and how we want to control what it can do. It can be an IP address or domain name, for example.\nWhat can the server access? The more limited that is, the better. It should probably be capable of logging somewhere, but it needs only limited DNS and internet access. There are likely to be integrations with other systems, but these should be enumerated and controlled.\nWhen can the server initiate contact? This seems less relevant to restrict, because logging and many integrations can be operational at any time.\nWhy is this access needed? As said earlier, any compute environment must be able to connect to certain other services for its functional operation. Yet that does not imply that it needs broad uncontrolled network access. And there are known cases of abusing overly permissive egress.\nHow can this rule be enforced? One way would be at the outer perimeter of the network, although it is also conceivable to do it through a dedicated firewall or a (network) security group in a cloud environment.\nWhat is the benefit here? In this example we have looked at a specific case of data that we want to protect. The Zero Trust approach and the Kipling method led us to various options that we have. These options allow us to hammer out implicit trust in an existing application.\nThe nice thing is that we can rate and rank these options based on their effectiveness, cost, and feasibility, all while focusing on one specific strategic asset that we want to protect. This is in stark contrast to a traditional approach where you start with strengthening the perimeter and just hope that this will have an impact on the one application that you want to protect first.\nImplication for maturity What we can also illustrate with this story is that, even though we can start with an individual application, many of these controls will become better and cheaper if there is some maturity and shared services in the organization. Reliable user identities are helped by proper federated identity management. Fine grained network access rules are easier to do if there is more software defined networking.\nMaturity is an investment, but it pays off in cheaper, faster and better security. That is another tradeoff that this example illustrates.\nFor more information on Zero Trust, visit https://cczt.clubcloudcomputing.com\nAlso published at https://www.linkedin.com/pulse/retrofitting-zero-trust-existing-application-peter-hj-van-eijk-zjpte\n","permalink":"/book/risk/retrofitting-zero-trust-existing-application/","title":"Retrofitting Zero Trust on an existing application: an illustration"},{"content":"How Digital Infrastructures at scale create value, power, and risk: a field guide The internet is a digital infrastructure at scale. Let’s unpack that for a moment.\nIt is digital, because it moves bits between devices.\nIt is at scale, because it moves these bits between billions of devices ranging from websites to mobile phones to robot lawn mowers. (how many?) That is impressive technology, but what keeps that together is the infrastructure part of it. That is what defines the relation between the consumers and the providers, and that is what we are exploring here, as I will show you later.\nLooking at it through the lens of infrastructures allows us to understand use cases, abuse cases, and how to define, build and manage things like the internet.\nThese digital infrastructures are everywhere, and they are important tools to create value and power. But at the same time, even because of this, they can also be great sources of risk.\nBetter understanding digital infrastructures at scale allows us to better navigate the digital world at large.\nSo, what are some of the characteristics of infrastructures? What is it that I am calling a digital infrastructure?\nWe are taking the internet as an example of a digital infrastructure because most people are familiar with it.\nSocial media is another example, and one that uses the internet as a component. Similarly digital marketplaces are an example. In the same way, the internet is also built on many other infrastructures: telecommunications, DNS, Number authorities. In the realm of IT management, there are many more examples of shared services. Because there are so many examples of digital infrastructures, it makes sense to explore the commonalities. That will make it a lot easier to create, analyze, control, and manage them.\nHow do the examples that I have mentioned deliver value to their users and other stakeholders? What are the mechanisms and principles in there? If we know a bit more about that, we could better engineer that value and the way it is delivered.\nWhat are the power structures around digital infrastructures? Who are the stakeholders that influence them, or are influenced by them? This cuts both ways: it brings value and risk to the table. Stakeholders typically want to increase value and reduce risk to them. But interests may also be conflicting, and resources are never infinite. With more understanding of these power structures, we have another handle on value creation and risk reduction.\nWhat are the risks associated with digital infrastructures, and what are the ways to organize our management of them? Obviously, there is a vast body of knowledge around that, for example in the space of cloud computing, and to an extent also around artificial intelligence. I am not going to treat all of that, instead I want to bring out some core principles that have helped me, my clients, and my students to navigate that.\nI think this story is of interest to many senior roles in IT and business. I have worked with consultants, policymakers, researchers, risk assessors, IT architects, digital business owners, students, teachers and other scholars. Giving these people some tools to use for their work would be gratifying.\nI don’t have final answers to all the questions that I can pose, and I cannot afford to wait until I have them. For that reason, I want to present the book as an ongoing project, for the time being, where I organize my thoughts and research in the form of blog posts, that at some point will be assimilated in the draft book.\n","permalink":"/book/intro/book-intro/","title":"Book Intro"},{"content":"A diagram should automagically appear here.\n%% icons from https://iconify.design architecture-beta group api(Storage)[API] service db(database)[Database] in api service disk1(disk)[Storage] in api service disk2(disk)[Storage] in api service server(server)[Server] in api db:L -- R:server disk1:T --\u0026gt; B:server disk2:T -- B:db block-beta in space llm:2 space out in[\u0026#34;Prompt\u0026#34;] --\u0026gt; llm[\u0026#34;Large Language Model\u0026#34;] llm--\u0026gt; out[\u0026#34;Completion\u0026#34;] style in fill:#fff,color:#000,line:#000;stroke-width:0px,color:#000,stroke-dasharray: 5 5 style out fill:#fff,color:#000,line:#000;stroke-width:0px,color:#000,stroke-dasharray: 5 5 ","permalink":"/posts/mermaid-example/","title":"A mermaid diagram example"},{"content":"Public cloud migrations come in different shapes and sizes, but I see three major approaches. Each of these has very different technical and governance implications.\nThree approaches Companies dying to get rid of their data centers often get started on a ‘lift and shift’ approach, where applications are moved from existing servers to equivalent servers in the cloud. The cloud service model consumed here is mainly IaaS (infrastructure as a service). Not much is outsourced to cloud providers here. Contrast that with SaaS.\nThe other side of the spectrum is adopting SaaS solutions. More often than not, these trickle in from the business side, not from IT. These could range from small meeting planners to full blown sales support systems.\nMore recently, developers have started to embrace cloud native architectures. Ultimately, both the target environment as well as the development environment can be cloud based. The cloud service model consumed here is typically PaaS.\nI am not here to advocate the benefits of one over the other, I think there can be business case for each of these.\nThe categories also have some overlap. Lift and shift can require some refactoring of code, to have it better fit cloud native deployments. And hardly any SaaS application is stand alone, so some (cloud native) integration with other software is often required.\nProfound differences The big point I want to make here is that there are profound differences in the issues that each of these categories faces, and the hard decisions that have to be made. Most of these decisions are about governance and risk management.\nWith lift and shift, the application functionality is pretty clear, but bringing that out to the cloud introduces data risks and technical risks. Data controls may be insufficient, and the application’s architecture may not be a good match for cloud, leading to poor performance and high cost.\nOne group of SaaS applications stems from ‘shadow IT’. The people that adopt them typically pay little attention to existing risk management policies. These can also add useless complexity to the application landscape. The governance challenges for these are obvious: consolidate and make them more compliant with company policies.\nAnother group of SaaS applications is the reincarnation of the ‘enterprise software package’. Think ERP, CRM or HR applications. These are typically run as a corporate project, with all its change management issues, except that you don’t have to run it yourself.\nThe positive side of SaaS solutions, in general, is that they are likely to be cloud native, which could greatly reduce their risk profile. Of course, this has to be validated, and a minimum risk control is to have a good exit strategy.\nFinally, cloud native development is the most exciting, rewarding and risky approach. This is because it explores and creates new possibilities that can truly transform an organization.\nOne of the most obvious balances to strike here is between speed of innovation and independence of platform providers. The more you are willing to commit yourself to an innovative platform, the faster you may be able to move. The two big examples I see of that are big data and internet of things. The major cloud providers have very interesting offerings there, but moving a fully developed application from one provider to another is going to be a really painful proposition. And of course, the next important thing is for developers to truly understand the risks and benefits of cloud native development.\nAgain, big governance and risk management issues to address.\n","permalink":"/book/value/three-ways-to-go-cloud/","title":"Multiple Ways to Go Cloud"},{"content":"🔎 Type to search through blog posts and book chapters.\n","permalink":"/search/","title":"Search"}]