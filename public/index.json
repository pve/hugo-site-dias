[{"content":"","title":"Chapter 2","url":"/book/chapter-2/"},{"content":"Text for chapter 1\n","title":"Chapter 1","url":"/book/chapter-1/"},{"content":"Here is the draft design of the graphic novel version of this book, which may or may not happen..\n","title":"Digital Infrastructures - the graphic novel","url":"/posts/graphic-novel-page1/"},{"content":"One of the key aspirations of Digital Infrastructures at Scale is to equip you with the tools to shape and drive change in your professional environment‚Äîespecially when your goal is to lead a transformation.\nTwo important positions of influence in IT are architects and auditors/assessors.\nIT architects are builders. They design new applications, platforms, and infrastructures that enable businesses to operate more effectively. A CRM system, for example, is not just a technical solution‚Äîit transforms workflows, communication, and decision-making. Similarly, infrastructure architects create digital foundations that accelerate the deployment of such applications.\nAuditors and risk assessors, on the other hand, focus on preventing failure. Every technology introduces risks, and without proper attention, these risks can derail progress‚Äîwhether through security breaches, compliance fines, or operational breakdowns.\nBoth roles involve power. Without power, things will not move in the way you want, and every initiative encounters resistance. Understanding the sources of power‚Äîand the forces that push back‚Äîis essential for you to achieve meaningful outcomes.\nMoney is a form of power, but it is often a blunt instrument. In organizations, other forces‚Äîsuch as survival and strategic priorities‚Äîcan be even more influential. The key to driving transformation is linking these forces together with precision.\nThis book provides you with the frameworks, insights, and real-world examples to navigate these challenges and drive successful change.\n","title":"Positions of Power in IT","url":"/posts/positions-of-power-it/"},{"content":"The AI landscape has many digital infrastructures.\nLet\u0026rsquo;s explain this step by step and focus on which data is stored where, and how it is processed.\nA core element of AI systems is a trained model. At least that is true for the dominant AI form these days: deep learning neural networks.\nA trained model is the result of processing a lot of training data by a specific neural network. These models are fixed in size, but typically very big. The smallest useful models are close to a Gigabyte, while recent public chat models run into multiple Terabytes.\nA trained language model is effectively a piece of software. You feed it text, and text comes out. This is called inferencing, and it requires that the entire model fit into RAM memory. Only for the most minimal cases is this possible on a high end laptop. For more interesting use cases you\u0026rsquo;ll need a lot more hardware. Therefore there is a good business case for sharing this across many users.\nblock-beta space in space llm:2 space out space in[\u0026#34;Prompt\u0026#34;] --\u0026gt; llm[\u0026#34;Large Language Model\u0026#34;] llm--\u0026gt; out[\u0026#34;Completion\u0026#34;] style in fill:#fff,color:#000,line:#000;stroke-width:0px,color:#000,stroke-dasharray: 5 5 style out fill:#fff,color:#000,line:#000;stroke-width:0px,color:#000,stroke-dasharray: 5 5 In cloud terms, we would call this Inferencing as a Service. In the NIST model, we\u0026rsquo;d call this SaaS or PaaS, depending on whether is it consumed directly or as part of a larger application.\nThis gives us the option to look for this on the market, or create a shared service inside an organisation or group of organisations (e.g. private / community cloud). It also implies that we need to have a clear definition of the service and related service level objectives. This includes any allocation of security responsibilies.\n","title":"What are the AI digital infrastructures?","url":"/posts/digital-inf-ai/"},{"content":"Building Our Own Cloud Kootwijk: Rethinking Digital Sovereignty In the Netherlands, we are currently engaged in a heated debate about the undesirable dominance of big tech, particularly over a significant portion of the digital infrastructure of the Dutch government. This includes email, file storage, and many other forms of digital storage and processing‚Äîmost of which are handled by American big tech companies.\nI am sure a similar debate is going on in many other countries.\nGiven this reality, what can we do about it? How can we build our own \u0026ldquo;Cloud Kootwijk\u0026rdquo;?\nExploring Alternatives Setting up a comprehensive, government-funded solution does not seem like a viable approach. History has shown many failures in such attempts. Moreover, our current procurement processes do not lend themselves to agility. Due to the scale involved, any such initiative would likely end up with a large, well-established company.\nThis brings us to another challenge: every big tech company is, or ultimately becomes, an American company. The two largest software markets in the world are the U.S. and Germany. Even the largest German IT firms cannot afford to ignore the American market and are often listed on U.S. stock exchanges.\nAs a result, every big tech company‚Äîdirectly or indirectly‚Äîfalls under the control of the U.S. government, regardless of its political orientation at any given time.\nRethinking the Approach With this in mind, we need to rethink our strategy. If big tech is not the answer, how can we organize \u0026ldquo;small tech\u0026rdquo; in a way that makes it a viable alternative for a large-scale buyer?\nWe are talking about digital infrastructures that are essential to the functioning of our society. These infrastructures have a certain \u0026ldquo;commodity\u0026rdquo; nature and do not primarily revolve around innovative applications. In principle, they should be deliverable by multiple companies.\nThere is no fundamental barrier to IT services being provided by a consortium of service providers. In fact, this is how the Internet has operated for decades. Its core consists of open standards that facilitate a market of service providers with sustainable business models. To prevent monopolization, these markets need some level of regulation, including enforcing open standards for interoperability and portability. In the case of strategic autonomy, additional rules regarding ownership of these companies may also be necessary.\nWhile having much of the underlying software available as open source would help, it is not essential. Open source is neither a sufficient nor a necessary condition for interoperability and portability.\nOpen standards (ideally with open-source reference implementations) are more important. They also contribute to another critical element of \u0026ldquo;small tech\u0026rdquo;: building sufficient expertise.\nKnowledge as the Foundation of Autonomy In my view, IT services consist of hardware, software, and systems management . Management can be broken down into processes and skilled personnel. Most of these elements are either commodities or readily available.\nFinding enough personnel competent in the relevant technologies is more challenging, but not impossible.\nTo achieve a sufficient level of autonomy in digital infrastructures, we need to develop and maintain this expertise. The best way I know to do this is to start small and scale up. Expanding by a factor of ten at each step is an ambitious yet achievable goal. This points towards managing our digital infrastructure through a consortium of smaller companies. The VNG‚Äôs Common Ground and Haven initiatives offer promising examples in this direction.\nA Call to Action I challenge both policymakers and society to launch a production pilot using a fully open-source office automation environment. I believe we could select the necessary open-source components for such a pilot in an afternoon. Managing this at a scale of 1,000 active users would be an excellent starting ambition. This will take months‚Äîperhaps longer‚Äîbut if successful, it will provide a blueprint for a \u0026ldquo;Cloud Kootwijk\u0026rdquo; should the need become urgent.\nWill this be painful? Yes. But not as painful as realizing too late that we have lost our democracy and prosperity to a foreign power‚Äîor worse, to a group of multinational corporations that are not even under the control of any state.\n","title":"How Can We Achieve Autonomy Over Our Digital Infrastructures?","url":"/posts/how-can-we-achieve-autonomy/"},{"content":"This is hardly a book on digital technology, but more on the organisational and societal impact of managing this at scale.\nFor a better understanding of digital technology I highly recommend Brian Kernighan‚Äôs ‚ÄúUnderstanding the Digital World‚Äù. () And if you want to have fun while learning how to understand software, I‚Äôd highly recommend ‚ÄúThe Nature of Code‚Äù by Daniel Schiffman ().\nAs you will find out in this book, I am putting on some very specific lenses to look at the world. You may find them restrictive; you may find them arbitrary(?), you may find them opiniated. Yes, this specific way of looking at the effects of digital technology has its limits. However, the essence of understanding is to be able to extract the core elements out of something. Paradoxically, only by restricting the way we look at things can we see more clearly. Cut through the noise. Like Goethe said: ‚ÄúIn der Beschr√§nkung zeigt sich erst der Meister‚Äù, though he applied that to a different subject.\nI have a very limited perspective of the world. And yet, that limitation has served me well. It has allowed me to focus on core principles that you may not see if you take in all the richness and idiosyncrasies. My limited perspective can serve you as a map. And you know that every map is not equal to the terrain it covers. In fact, the purpose of the map is to extract only the most relevant features of the terrain. How good a map is, judgement on the quality of the map is not objective. It is dependent on what you use the map for. A railroad map is close to useless if you navigate by car or boat. That is why you should consider the models and the perspectives that I give you as maps to guide your journey. If they bring you where you want to be, great! If not, get a different map.\nAs you will see, the core elements of the models I find useful revolve around interactions between autonomous agents that process information. These agents typically have limited (bounded) capabilities. (link to maps as means to reduce attention requirements). These agents can be humans, software or AI agents.\nAs I believe that you are an autonomous agent as well, it is up to you to decide how useful these models and stories are for your practice. You will not find answers to every question that you have around digital infrastructures. Where possible and relevant I will point you to other views, stories and explanations. At the same time, you may find my maps to be also useful for domains that are outside the digital.\n","title":"Preface","url":"/posts/preface/"},{"content":"Zero Trust Architecture is an approach to better cybersecurity. To many, it seems daunting to implement. But it does not have to be hard to start.\nConsider this hypothetical situation.\nYou have an application with hundreds of thousands of sensitive records, let‚Äôs say client records. We assume that in this example it seems hard to implement MFA (Multi Factor Authentication) on it. What other controls can you implement to reduce the assumed trust? We can use the Kipling method, which is at the core of Zero Trust architectures, to engineer better controls. In short, the Kipling method is about the \u0026lsquo;who\u0026rsquo;, \u0026lsquo;what\u0026rsquo;, \u0026lsquo;when\u0026rsquo;, etcetera of allowed communication.\narchitecture-beta group exec(server)[Execution environment] service users(cloud)[Cloud Users] service fw[Firewall] service app(database)[App] in exec users:R --\u0026gt; L:fw fw:R --\u0026gt; L:app We want to allow specific access for specific use cases and be explicit about it. However, to focus our efforts, it makes sense to also identify that our biggest risk is the exfiltration of a lot of those sensitive records: a data breach.\nThe allow rules Let‚Äôs begin with the ‚Äòwho.‚Äô Who is accessing the information? The application in our example probably does authentication and authorization of users. Where do these come from, and how accurate is that information? Ideally, this comes from an up-to-date corporate directory, but even if it‚Äôs not, you can, for example, ask how quickly a departing user has their access revoked.\nThen the ‚Äòwhat‚Äô. Which application and resources are we accessing? What are specific users allowed to do? As part of operational risk management, we have probably already identified who, based on their role, can modify records or do other critical actions. But think one step further. As our main fear is exfiltration of large amounts of data, we can look at where we can control that. Maybe we can disallow large downloads, for example, except when a timely ‚Äòfour eyes‚Äô approval is in place.\n(By the way, I think it makes sense to look at large downloads as a risk separate from small data leaks. After all, many employees have access to small amounts of information, and that risk is typically already accepted).\nMoving on to the ‚Äòwhen‚Äô. It can reduce risk to limit access to specific times of the day for most users. There is not necessarily a reason to allow more.\nNext is the ‚Äòwhere‚Äô. Where are the users and the application located? Our case description does not give us a lot of information here. The application could be a server, which then has an IP address. And through a firewall combined with a geographical information feed, we may be able to restrict access to users based on their location.\nThe ‚Äòwhy‚Äô stands for the business reason. In this case, our data is sensitive, and we don‚Äôt want it to be exfiltrated. And in our analysis, we have identified several possible controls.\nHow: One layer of enforcement of this would be in the application itself. In the Zero Trust jargon, we are putting a Policy Enforcement Point in the application logic.\nThe mirror allow rule However, there is also another set of ‚Äòallow rules‚Äô that is often overlooked. I am tempted to call these the ‚Äòmirror‚Äô rules, but it is not a standard Zero Trust term.\nThe data and the application reside in a compute environment, for example a server. There may be malware running in that server. After all, one of the tenets of Zero Trust is ‚Äòassume breach‚Äô. If the server has permission to access anything on the internet, that malware might easily exfiltrate large amounts of data.\nIn addition to looking at the user accessing the data, we are looking at the place where the data resides and see if its compute environment can access the world, and thus exfiltrate data. This is the mirror image of the first rule: you could say that subject and target are switched around.\nWe can apply the same Kipling method from the perspective of that server (or any compute environment that has the data in it). What is it allowed to do?\nThe ‚Äòwho‚Äô then is the server, and there are various ways to identify it, depending on how it is set up and how we want to control what it can do. It can be an IP address or domain name, for example.\nWhat can the server access? The more limited that is, the better. It should probably be capable of logging somewhere, but it needs only limited DNS and internet access. There are likely to be integrations with other systems, but these should be enumerated and controlled.\nWhen can the server initiate contact? This seems less relevant to restrict, because logging and many integrations can be operational at any time.\nWhy is this access needed? As said earlier, any compute environment must be able to connect to certain other services for its functional operation. Yet that does not imply that it needs broad uncontrolled network access. And there are known cases of abusing overly permissive egress.\nHow can this rule be enforced? One way would be at the outer perimeter of the network, although it is also conceivable to do it through a dedicated firewall or a (network) security group in a cloud environment.\nWhat is the benefit here? In this example we have looked at a specific case of data that we want to protect. The Zero Trust approach and the Kipling method led us to various options that we have. These options allow us to hammer out implicit trust in an existing application.\nThe nice thing is that we can rate and rank these options based on their effectiveness, cost, and feasibility, all while focusing on one specific strategic asset that we want to protect. This is in stark contrast to a traditional approach where you start with strengthening the perimeter and just hope that this will have an impact on the one application that you want to protect first.\nImplication for maturity What we can also illustrate with this story is that, even though we can start with an individual application, many of these controls will become better and cheaper if there is some maturity and shared services in the organization. Reliable user identities are helped by proper federated identity management. Fine grained network access rules are easier to do if there is more software defined networking.\nMaturity is an investment, but it pays off in cheaper, faster and better security. That is another tradeoff that this example illustrates.\nFor more information on Zero Trust, visit https://cczt.clubcloudcomputing.com\nAlso published at https://www.linkedin.com/pulse/retrofitting-zero-trust-existing-application-peter-hj-van-eijk-zjpte\n","title":"Retrofitting Zero Trust on an existing application: an illustration","url":"/posts/retrofitting-zero-trust-existing-application/"},{"content":"How Digital Infrastructures at scale create value, power, and risk. A field guide The internet is a digital infrastructure at scale. Let‚Äôs unpack that for a moment.\nIt is digital, because it moves bits between devices.\nIt is at scale, because it moves these bits between billions of devices ranging from websites to mobile phones to robot lawn mowers. (how many?) That is impressive technology, but what keeps that together is the infrastructure part of it. That is what defines the relation between the consumers and the providers, and that is what we are exploring here, as I will show you later.\nLooking at it through the lens of infrastructures allows us to understand use cases, abuse cases, and how to define, build and manage things like the internet.\nThese digital infrastructures are everywhere, and they are important tools to create value and power. But at the same time, even because of this, they can also be great sources of risk.\nBetter understanding digital infrastructures at scale allows us to better navigate the digital world at large.\nSo, what are some of the characteristics of infrastructures? What is it that I am calling a digital infrastructure?\nWe are taking the internet as an example of a digital infrastructure because most people are familiar with it.\n","title":"Book Intro","url":"/posts/book-intro/"},{"content":"A diagram should automagically appear here.\n%% icons from https://iconify.design architecture-beta group api(Storage)[API] service db(database)[Database] in api service disk1(disk)[Storage] in api service disk2(disk)[Storage] in api service server(server)[Server] in api db:L -- R:server disk1:T --\u0026gt; B:server disk2:T -- B:db block-beta in space llm:2 space out in[\u0026#34;Prompt\u0026#34;] --\u0026gt; llm[\u0026#34;Large Language Model\u0026#34;] llm--\u0026gt; out[\u0026#34;Completion\u0026#34;] style in fill:#fff,color:#000,line:#000;stroke-width:0px,color:#000,stroke-dasharray: 5 5 style out fill:#fff,color:#000,line:#000;stroke-width:0px,color:#000,stroke-dasharray: 5 5 ","title":"A mermaid diagram example","url":"/posts/mermaid-example/"},{"content":"Public cloud migrations come in different shapes and sizes, but I see three major approaches. Each of these has very different technical and governance implications.\nThree approaches Companies dying to get rid of their data centers often get started on a ‚Äòlift and shift‚Äô approach, where applications are moved from existing servers to equivalent servers in the cloud. The cloud service model consumed here is mainly IaaS (infrastructure as a service). Not much is outsourced to cloud providers here. Contrast that with SaaS.\nThe other side of the spectrum is adopting SaaS solutions. More often than not, these trickle in from the business side, not from IT. These could range from small meeting planners to full blown sales support systems.\nMore recently, developers have started to embrace cloud native architectures. Ultimately, both the target environment as well as the development environment can be cloud based. The cloud service model consumed here is typically PaaS.\nI am not here to advocate the benefits of one over the other, I think there can be business case for each of these.\nThe categories also have some overlap. Lift and shift can require some refactoring of code, to have it better fit cloud native deployments. And hardly any SaaS application is stand alone, so some (cloud native) integration with other software is often required.\nProfound differences The big point I want to make here is that there are profound differences in the issues that each of these categories faces, and the hard decisions that have to be made. Most of these decisions are about governance and risk management.\nWith lift and shift, the application functionality is pretty clear, but bringing that out to the cloud introduces data risks and technical risks. Data controls may be insufficient, and the application‚Äôs architecture may not be a good match for cloud, leading to poor performance and high cost.\nOne group of SaaS applications stems from ‚Äòshadow IT‚Äô. The people that adopt them typically pay little attention to existing risk management policies. These can also add useless complexity to the application landscape. The governance challenges for these are obvious: consolidate and make them more compliant with company policies.\nAnother group of SaaS applications is the reincarnation of the ‚Äòenterprise software package‚Äô. Think ERP, CRM or HR applications. These are typically run as a corporate project, with all its change management issues, except that you don‚Äôt have to run it yourself.\nThe positive side of SaaS solutions, in general, is that they are likely to be cloud native, which could greatly reduce their risk profile. Of course, this has to be validated, and a minimum risk control is to have a good exit strategy.\nFinally, cloud native development is the most exciting, rewarding and risky approach. This is because it explores and creates new possibilities that can truly transform an organization.\nOne of the most obvious balances to strike here is between speed of innovation and independence of platform providers. The more you are willing to commit yourself to an innovative platform, the faster you may be able to move. The two big examples I see of that are big data and internet of things. The major cloud providers have very interesting offerings there, but moving a fully developed application from one provider to another is going to be a really painful proposition. And of course, the next important thing is for developers to truly understand the risks and benefits of cloud native development.\nAgain, big governance and risk management issues to address.\n","title":"Multiple Ways to Go Cloud","url":"/posts/three-ways-to-go-cloud/"},{"content":"One of the key aspirations of Digital Infrastructures at Scale is to equip you with the tools to shape and drive change in your professional environment‚Äîespecially when your goal is to lead a transformation. Two key positions of influence in IT are architects and auditors/assessors. IT architects are builders. They design new applications, platforms, and infrastructures that enable businesses to operate more effectively. A CRM system, for example, is not just a technical solution‚Äîit transforms workflows, communication, and decision-making. Similarly, infrastructure architects create digital foundations that accelerate the deployment of such applications. Auditors and risk assessors, on the other hand, focus on preventing failure. Every technology introduces risks, and without proper attention, these risks can derail progress‚Äîwhether through security breaches, compliance fines, or operational breakdowns. Both roles involve power. Change does not happen in isolation, and every initiative encounters resistance. Understanding the sources of power‚Äîand the forces that push back‚Äîis essential for you to achieve meaningful outcomes. Money is a form of power, but it is often a blunt instrument. In organizations, other forces‚Äîsuch as survival and strategic priorities‚Äîcan be even more influential. The key to driving transformation is linking these forces together with precision. This book provides you with the frameworks, insights, and real-world examples to navigate these challenges and drive successful change.\n","title":"","url":"/posts/tmp/"},{"content":"üîé Type to search through blog posts and book chapters.\n","title":"Search","url":"/search/"}]