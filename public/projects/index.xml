<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Projects on Digital Infrastructures at Scale</title>
    <link>http://localhost:1313/projects/</link>
    <description>Recent content in Projects on Digital Infrastructures at Scale</description>
    <generator>Hugo -- 0.148.0</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 29 Aug 2025 09:33:34 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/projects/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>OWASP LLM Risk Allocation</title>
      <link>http://localhost:1313/projects/olra/olra/</link>
      <pubDate>Fri, 29 Aug 2025 09:33:34 +0000</pubDate>
      <guid>http://localhost:1313/projects/olra/olra/</guid>
      <description>&lt;p&gt;Applications based on LLMs (Large Language Models) have risks too.
The &lt;a href=&#34;https://owasp.org/www-project-top-10-for-large-language-model-applications/&#34;&gt;OWASP Top 10 for LLM Applications&lt;/a&gt; risks are a good start for analyzing the risks of such a system.&lt;/p&gt;
&lt;p&gt;These types of applications, like many others, are also cloud applications.
This means that there is a variety of parties responsible for controlling those risks.&lt;/p&gt;
&lt;p&gt;But, who is supposed to do each control? And which role do they have?
For example, there are model providers, there is the AI consumer, and so on.
For a deeper story on these roles look at &lt;a href=&#34;http://localhost:1313/book/diginfra/ai-roles/&#34;&gt;AI roles&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
