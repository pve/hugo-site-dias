---
date: '2025-03-12T13:13:35Z'
draft: false
title: 'Introduction to Risk'
weight: 2
---

Risk is the flip side of value. For everything that is of value, there can be circumstances threatening that value.
While value is realized in the past and the present, risk is what can happen with that value in the *future*.

Risk in a digital world is not always easy to think through. While we can borrow a lot from the real world, certain important differences exist.

At the core of every risk assessment there is the thing we worry about the most: the '**asset**'.
In a digital world, this is often the **data**. Think of business-critical data, like our database of customers. Think of data that we have a compliance obligation on, such as personal data.

In information security, like the name implies, we mainly worry about the data.

It is common to distinguish between *availability*, *confidentiality*, and *integrity* risks. All of these can be risks to the business.

Business processes run on data, and if they are not, they can typically be improved by using more data. We cover that in more detail in another place.

### Availability

If the data is not available, the business process can suffer, and its value can be reduced.

Let's have a look at some examples of availability.

>Your (mobile) phone is dependent on a network.
>If the network is unavailable, too far away, or congested, you have an availability problem, and the usefulness of having a phone to communicate drops to zero.
>
>As another example, consider a payment terminal: if it does not work, you cannot pay, and probably will not get what you wanted to buy.

### Confidentiality

If the data leaks out, the business process can suffer, and its value reduced.

Confidentiality is about keeping data secret. Again, the examples are not too hard to find. There are probably pictures on your phone that you do not want to share with the entire world.

In a business context, you don't want your competitors to know about your plans and pricing strategies.

### Integrity

If the data does not reflect the reality well enough, the business process can suffer, and its value reduced.

Integrity means that the data is sufficiently accurate, complete, and consistent.

For example, if customer order records are missing, they may not receive their products. Or they are not invoiced.
That is a loss to the business.

Search results (or AI chatbots for that matter) can also lack integrity, for example if they report in a biased way, or leave out important answers.

Integrity is a more fluid concept than the others. What is quality data to somebody may be totally inaccurate for somebody else. Consider social media metrics such as "likes" and "shares." A marketer might see a high number of likes as valuable data indicating effective audience engagement. Meanwhile, a data analyst focused on conversion rates might regard likes as less meaningful, prioritizing click-through data and sales metrics as more accurate indicators of campaign success. Thus, while the 'likes' metric is accurate, its perceived quality and relevance differ based on business goals.
---
date: '2025-05-11T08:37:01Z'
draft: true
title: 'Information Security Assets'
weight: 5
---

Let's dive a little deeper into assets.
The most relevant asset in information security is data.
That is what the users of information most care about.
In addition, we can also see the processing power that we need as an asset.

Here are some examples of data assets:

- A customer record in a business system
- An MRI scan
- A browser cookie (on the server)
- A logfile entry

As you can guess from these examples, many of these have regulatory concerns attached to them, because of the type of data that they consist of.
One of the tasks of a risk analyst is to figure out what regulations apply exactly.

From a risk analysis, the asset is the unit that we consider.
ENISA, the European Network Information Security Agency, is one of the many institutes in the world described IT risk management.
Their [cloud security report](https://www.enisa.europa.eu/publications/cloud-computing-risk-assessment) identified a number of asset types.
A few examples to illustrate the breadth of that are:

- A4. Intellectual property. This represents value to the organization owning it.
- A6. Personal data
- A14. Cloud service management interface
- A17. Physical hardware
- A22. Security logs

But the document is still worth a more in depth study.

## Vulnerabilities and threats

Now what could *possibly* go wrong with a data asset?
Who might see your secret pictures?

Various words are used to talk about this.

ENISA talks about vulnerabilities as "Any circumstance or event with the potential to adversely impact an asset". It is therefore about the damage potential, not the actual damage.

Microsoft in their framework (details, fact check) uses these concepts.

- Threat. A potential occurrence, malicious or otherwise, that might damage or compromise your assets.
- Vulnerability. A weakness in some aspect or feature of a system that makes a threat possible. Vulnerabilities might exist at the network, host, or application levels.
- Attack (or exploit, leads to bad consequence). An action taken by someone or something that harms an asset. This could be someone following through on a threat or exploiting a vulnerability.

Again, it is about the way that this leads to damage.
Threats and vulnerabilities are hypothetical, they are about things that have not happened yet.
That is, they have not happened to *you*, yet.
And that makes them hard to reason about.
We'll return to that.

Here are some cloud specific examples from the ENISA report

- V1. AAA vulnerabilities (i.e. weak or reused passwords)
- V4. Remote access to management interface
- V5. Hypervisor vulnerabilities

An example of the first one is a weak password like "password123", especially if it is used on many accounts.
V4 is about cloud access. In a cloud environment, you don't walk up to your computer, you log in to it remotely, possibly over the internet.
That brings new vulnerabilities. Especially since the 'management interface' is not just an account, it is an account that rules other accounts.

Finally, a more technical cloud specific example is the hypervisor.
The hypervisor is a technology for dividing up a physical machine into multiple virtual machines.
Each of these could serve different customers.
If you are one of those customers, you typically don't want another customer to have access to your data.
The hypervisor is an 'isolation provider', and it creates isolated 'execution environments' (see elsewhere).
Failure to isolate is thus a vulnerability.

## Risk

Vulnerabilities exist all over the place.
All technology has weaknesses.
But what are the ones that matter?

Most vulnerabilities are not easy to exploit, and they are not necessarily leading to a lot of damage.
For example: while hypervisor vulnerabilities exist, they are really esoteric and hard to exploit.
Most of them have been fixed anyway.
And losing your drivers license as it slips out of your pocket is a nuisance, but not something that ends the world.

What matters is how big the resulting damage can be, and what the chances are that it happens.

As we saw earlier, it is common to distinguish three major sources of damage: lack of availability, lack of integrity, lack or confidentiality.

Each of these hints at a business process in which the information is used.
But it is nothing more than a hint.
Part of the job of a risk analyst is to figure out what kinds of damages can happen in real use cases.
The value of that the damage is called the *impact*, and it mostly relates to what happens outside the information system.
It happens to another stakeholder.
If in a risk analysis you cannot find a credible stakeholder who is impacted, it is not much of a risk.

As risk is always about the future, it often makes sense to think in terms of probabilities.
One way to quantify risks is to assign them a value based on impact and probability.
The aforementioned ENISA report mentions this formula.

>Risk = Impact * Probability

Even without the details of this formula 
ENISA example
For example:

A14. Cloud service management interface
+
V1. AAA vulnerabilities
Leads to
R.11 Management interface compromise

---
date: '2025-04-15T21:27:25Z'
draft: false
title: 'Who Suffers?'
weight: 10
---

I have found that no discussion on risk is going to lead anywhere if it does not make clear who suffers from it.
Make clear who has the pain.
For my phone and laptop it is easy: if I lose them, I suffer.
In a larger organization it is less clear.
Suppose a server dies.
Whose application then no longer runs?
Who has to pay for a new server?
This gets increasingly harder if we are talking about shared services, because the owner and the consumer are now decoupled.

A related complication pops up when a failing component or service is remote from the organizations main purpose, or mission.
We see this a lot around software vulnerabilities.
A software vulnerability can allow a malicious actor to fully take over a service.
But unless you demonstrate that:

1. this vulnerability can actually be exploited, and
1. this exploit can cause damage to the mission

it would be a waste of time to address it individually.

If you are involved in risk analysis, make sure to complete the reasoning up to the point where the stakeholder with the budget feels the pain.
To do less is a waste of resources.

## External victims

It gets a little more complicated if those who suffer are not part of the organization that controls that risk.
Example: as a merchant accepting credit card payments of your customers, the customers credit card number helps you get paid.
But if you accidentally disclose it, and it falls in the hands of a bad actor, you don't suffer.
The customer does, or their bank.

From the perspective of the customer, your activity impacts something that is of value to them.
Economists call this an externality, and we see various examples of that in the world of risk, and in digital infrastructures at large.

As the example customer, there is little that you can do about it.
Why the credit card system still works is that banks and credit card issuers have an interest in the system working, and they don't want to saddle either the card holder or themselves with these losses.

They have the pain, and their response has been to create a mandatory certification scheme for everybody who touches a credit card number.
You cannot process a credit card, and definitely not at scale, without being PCI/DSS certified.
This is enforced by contract.
If you can't abide, you can't process credit cards, which may be important to you.
This is how they make the credit cards processors suffer.
It is an industry regulation.

Similarly, personal data loss is a risk to the person, not to the organization that collects your personal data.
That is why there are privacy regulations, of which the EU GDPR (General Data Protection Regulation) is the most well-known.

These aren't industry regulations, they are the law.
And if you don't follow the law, countries can fine you.
Unfortunately, as I mention elsewhere, money is a blunt instrument.
So while privacy laws work, they are not a perfect solution.

Summing up, this leads to a new category of risk: compliance risk.
If you do not comply with regulations, whether they are industry or government, you will suffer the pain.
---
date: '2025-05-19T21:39:41Z'
draft: true
title: 'Controls'
weight: 15
---
## Controls

Microsoft: Countermeasure (or control). A safeguard that addresses a threat and mitigates risk.

To be continued.

what other assets?

Where did that file go?
Where did you leave that address?
Who changed your password?

Once we have assets and risks identified, we think of ways to minimize these risks.
Those ways are called controls or measures.
If you have read through an IT risk management book, many of these should be familiar.
For the rest of you, here is a brief outline of some control examples for data.

- Classification: organizing data in various categories. This could from public to top secret, with a few in between. This is not really a technical control, but rather a means to apply specific controls on that data.
- Labelling: similar to classification. Data labels can relate data to specific use cases, such as development versus production, specific applications or owners.

One neat way to organize risks is according to a data lifecycle model.

- Create: classify, label
- Store: encryption
- Use: logical controls
- Share: DLP, encryption
- Archive: asset management
- Destroy: crypto shredding (encrypting and deleting the key)
---
date: '2025-05-13T09:39:02Z'
draft: true
title: 'Mitigation'
weight: 16
---

walk in with well spelled out risks, a clear connection to business impact, data, a mitigation plan, and measurable targets.

Providing third-party assurance enables the board to deliver on its risk management responsibilities.
---
date: '2025-03-09T18:54:30Z'
draft: false
title: 'Risks of digital infrastructures'
weight: 20
---
---
date: '2025-05-01T07:40:07Z'
draft: true
title: 'What about AI Risks?'
weight: 20
notes: whatever
---

While Artificial Intelligence, especially the generative type, is a highly disruptive form of IT innovation, its risk management still follows the same basic principles.
We just need to extend those principles to new forms of data and software.

- What are the assets?
- What are the vulnerabilities and threats to it?
- What is the damage that this can bring, realistically?

So let's see how what the AI specifics are.

## AI Assets

Let's focus on the AI assets that are the product of deep learning.
Deep learning neural networks have represented a shift in AI technology.
Before that, symbolic AI was more common.

The reason to elaborate on this is that there is a fundamental difference between symbolic AI and neural network AI.
Symbolic AI works by following programmatic rules, and is fairly deterministic.
Neural networks are the result of training with large datasets.
They have enabled *generative AI* which is much less deterministic.

Furthermore, and this is really significant from a security perspective, in its operation, GenAI mixes data and instruction to operate on that data deeply.

For example, anything an external user inputs to the system as data, might also be interpreted as an instruction.
This represents a huge vulnerability for hackers to exploit.

The trained models represent a lot of information, and that information is often hard to identify, and therefore to see the classic information risks of.

Models are effectively software: you stick data in them, and data comes out.
But because they also embody a lot of data (we are talking Gigabytes, Terabytes and beyond of training data) they combine the risks of software with the risks of data.

An AI system has a few more traditional and less traditional data assets associated with it.
Base models, or foundational models, such as Llama, often get additional training or finetuning.
The data used for that may well include proprietary or sensitive data.

Then there are system prompts and additional data sources that are fed into those models, together with user input.

## AI Vulnerabilities

## Damage---
date: '2025-05-05T10:18:17Z'
draft: true
title: 'What Is Trust'
weight: 25
---

Trust is an essential element in any collaboration.
We want to trust things, people, and organizations to do the right thing, to keep their promises.
Without trust, we will not be able to have confidence in the contribution of others, and that is a risk.

Anything we don't trust is a risk.
Anytime we assume trust that is not warranted is also a risk.
That is one of the core concepts of Zero Trust Security Architecture.

Trust can also be something that is fairly technical, as in: can I trust the webserver to be up all the time?

Trust is an elusive concept, but I do have a couple of ways of looking at it that I have found very useful, because they work in the real world.
One is from "the Prisoner's Dilemma," which is a story from game theory.
Another is "authority," which is basically "I trust somebody, because somebody tells me so".


---
date: '2025-05-27T19:58:41Z'
draft: false
title: 'Lean Risk and Economics'
weight: 40
---
From the moment a security vulnerability is discovered, it represents a negative value to its potential victims.
When it gets exploited, it can lead to loss of data or loss of integrity of the data.
This in turn impacts the victim's business processes.

For example, if personal data is leaked, reputations will be damaged, financial losses and fines can be expected. Credit card abuse forms another example of loss.

This "damage potential" increases as the vulnerability becomes well-known, progressing from nation state actors, to organized crime, to script kiddies, just to name one example pathway.
At first, few people know about it, but gradually more people will be able to inflict damage with it.
Over time, each step adds to the likelihood of that vulnerability being exploited and causing real damage.
The likelihood starts at near zero, and ends at close to 100% as the vulnerability is completely public.
This only stops when an investment is made to mitigate the vulnerability, for example by updating the software.
And hopefully, that investment is less costly than the damage potential.

This damage potential resembles the concept of "work in progress" from lean production.
Originally developed as the "Toyota Production System", it is also applied to software development.

For example, if your team spends three months with five staff members on building a feature, the work in progress is now valued at fifteen man-months.
That is an investment, which you will only recover once the feature is delivered and accepted.
Only then will you be paid for it, hopefully reflecting more value than you put in as an investment.
The mantra of lean production is to reduce work in progress.
Work in progress represents a liability: it is capital tied up, and maybe it won't bear fruit.
For example, your new piece of software may solve the wrong problem.
This is why entrepreneurs love the idea of a minimum viable product, it reduces work in progress.
You want to drop bad ideas as fast as possible.

Back to vulnerabilities:

The longer you wait with mitigating a vulnerability, the more negative value it accrues.
Fixing it represents an investment, which hopefully has a positive return.

There is a parallel with software development.
In software development, the net value drops as more money is poured into development, which then returns to positive as the feature is delivered and paid for (figuratively speaking).
In cyber security, the net value drops as the vulnerability is accessible to more threat actors, which then returns to zero after an investment is made into its mitigation.
Hopefully the cost of mitigation is less than the net value (i.e. damage potential) at that time.

This may sound pretty abstract, but I think I will be able to show the usefulness of this approach.
It helps us identify where and how investments are most productive in cyber security.

At any given moment in time, a stream of new vulnerabilities is thrown at cyber defenders around the world.
For example, in 2024 around 40.000 vulnerabilities were published in the CVE database ([source](https://www.cve.org/about/Metrics)).
Without additional information, all of these carry a negative value to an organization, as all of them have damage potential.

What investments can reduce the damage potential, and more importantly, which of these have the best return on investment?

Let's start at the beginning.
If you can match vulnerabilities to the software you have, you'll find a lot of them are not relevant as they relate to software you don't have.
A proper and accessible software inventory therefore represents an investment that can reduce the total negative value of vulnerabilities.

>Case in point: log4j affected a quarter of the world's organizations.
>So, without more information, your organization had a 25% chance of being pretty vulnerable.
>But, if you knew that this software was not in use anywhere, your vulnerability dropped to zero.

Then, fast forward through the value chain.
How can we reduce the negative value of a vulnerability?
One way is to reduce the damage potential through segmentation and similar Zero Trust techniques.

In a future extension of this unit, we can discuss:

- actor analysis: we have two types of actors here: malicious actors and potential victims, but there are more actors
- self inflicted vulnerabilities such as misconfigurations
- the value streams for malicious actors
- the value stream for security researchers
- the value streams for security service providers
- why Zero Trust reduces the negative value of a vulnerability.
---
date: '2025-02-28T14:15:21Z'
draft: false
title: 'Retrofitting Zero Trust on an existing application: an illustration'
weight: 60
---
Zero Trust Architecture is an approach to better cybersecurity. To many, it seems daunting to implement. But it does not have to be hard to start.

Consider this hypothetical situation.

You have an application with hundreds of thousands of sensitive records, let’s say client records. We assume that in this example it seems hard to implement MFA (Multi Factor Authentication) on it. What other controls can you implement to reduce the assumed trust? We can use the Kipling method, which is at the core of Zero Trust architectures, to engineer better controls. In short, the Kipling method is about the 'who', 'what', 'when', etcetera of allowed communication.

```mermaid
architecture-beta
group exec(server)[Execution environment]
    service users(cloud)[Cloud Users]
    service fw[Firewall]
    service app(database)[App] in exec


    users:R --> L:fw
    fw:R --> L:app
```

We want to allow specific access for specific use cases and be explicit about it. However, to focus our efforts, it makes sense to also identify that our biggest risk is the exfiltration of a lot of those sensitive records: a data breach.

## The allow rules

Let’s begin with the ‘who.’ Who is accessing the information? The application in our example probably does authentication and authorization of users. Where do these come from, and how accurate is that information? Ideally, this comes from an up-to-date corporate directory, but even if it’s not, you can, for example, ask how quickly a departing user has their access revoked.

Then the ‘what’. Which application and resources are we accessing? What are specific users allowed to do? As part of operational risk management, we have probably already identified who, based on their role, can modify records or do other critical actions. But think one step further. As our main fear is exfiltration of large amounts of data, we can look at where we can control that. Maybe we can disallow large downloads, for example, except when a timely ‘four eyes’ approval is in place.

(By the way, I think it makes sense to look at large downloads as a risk separate from small data leaks. After all, many employees have access to small amounts of information, and that risk is typically already accepted).

Moving on to the ‘when’. It can reduce risk to limit access to specific times of the day for most users. There is not necessarily a reason to allow more.

Next is the ‘where’. Where are the users and the application located? Our case description does not give us a lot of information here. The application could be a server, which then has an IP address. And through a firewall combined with a geographical information feed, we may be able to restrict access to users based on their location.

The ‘why’ stands for the business reason. In this case, our data is sensitive, and we don’t want it to be exfiltrated. And in our analysis, we have identified several possible controls.

How: One layer of enforcement of this would be in the application itself. In the Zero Trust jargon, we are putting a Policy Enforcement Point in the application logic.

## The mirror allow rule

However, there is also another set of ‘allow rules’ that is often overlooked. I am tempted to call these the ‘mirror’ rules, but it is not a standard Zero Trust term.

The data and the application reside in a compute environment, for example a server. There may be malware running in that server. After all, one of the tenets of Zero Trust is ‘assume breach’. If the server has permission to access anything on the internet, that malware might easily exfiltrate large amounts of data.

In addition to looking at the user accessing the data, we are looking at the place where the data resides and see if its compute environment can access the world, and thus exfiltrate data. This is the mirror image of the first rule: you could say that subject and target are switched around.

We can apply the same Kipling method from the perspective of that server (or any compute environment that has the data in it). What is it allowed to do?

The ‘who’ then is the server, and there are various ways to identify it, depending on how it is set up and how we want to control what it can do. It can be an IP address or domain name, for example.

What can the server access? The more limited that is, the better. It should probably be capable of logging somewhere, but it needs only limited DNS and internet access. There are likely to be integrations with other systems, but these should be enumerated and controlled.

When can the server initiate contact? This seems less relevant to restrict, because logging and many integrations can be operational at any time.

Why is this access needed? As said earlier, any compute environment must be able to connect to certain other services for its functional operation. Yet that does not imply that it needs broad uncontrolled network access. And there are known cases of abusing overly permissive egress.

How can this rule be enforced? One way would be at the outer perimeter of the network, although it is also conceivable to do it through a dedicated firewall or a (network) security group in a cloud environment.

## What is the benefit here?

In this example we have looked at a specific case of data that we want to protect. The Zero Trust approach and the Kipling method led us to various options that we have. These options allow us to hammer out implicit trust in an existing application.

The nice thing is that we can rate and rank these options based on their effectiveness, cost, and feasibility, all while focusing on one specific strategic asset that we want to protect. This is in stark contrast to a traditional approach where you start with strengthening the perimeter and just hope that this will have an impact on the one application that you want to protect first.

And we all know: hope is not a strategy.

## Implication for maturity

What we can also illustrate with this story is that, even though we can start with an individual application, many of these controls will become better and cheaper if there is some maturity and shared services in the organization. Reliable user identities are helped by proper federated identity management. Fine grained network access rules are easier to do if there is more software defined networking.

Maturity is an investment, and it pays off in cheaper, faster and better security. That is another tradeoff that this example illustrates.

For more information on Zero Trust, visit <https://cczt.clubcloudcomputing.com>

Also published at <https://www.linkedin.com/pulse/retrofitting-zero-trust-existing-application-peter-hj-van-eijk-zjpte>
